{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMg8px8C6n0E2MhuFmzfG+Z"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"tqnZ7zQ1_pxD","executionInfo":{"status":"ok","timestamp":1730639819849,"user_tz":-420,"elapsed":551,"user":{"displayName":"Hanh Mai","userId":"06375785904431938117"}}},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import re\n","import matplotlib.pyplot as plt\n","import nltk\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from nltk.tokenize import TweetTokenizer\n","from collections import defaultdict"]},{"cell_type":"code","source":["dataset_path = '/content/sentiment_analysis.csv'"],"metadata":{"id":"17YgoQzpAfbI","executionInfo":{"status":"ok","timestamp":1730639819849,"user_tz":-420,"elapsed":21,"user":{"displayName":"Hanh Mai","userId":"06375785904431938117"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["df = pd.read_csv(\n","    dataset_path,\n","    index_col = 'id')"],"metadata":{"id":"rZan5NXcAbUT","executionInfo":{"status":"ok","timestamp":1730639842699,"user_tz":-420,"elapsed":815,"user":{"displayName":"Hanh Mai","userId":"06375785904431938117"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# Data preproccessing\n","def text_normalize(text):\n","    # Retweet acronym \"RT\" removal\n","    text = re.sub(r'^RT[\\s]+', '', text)\n","\n","    # Hyperlinks removal\n","    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text)\n","\n","    # Hashtags removal\n","    text = re.sub(r'#', '', text)\n","\n","    # Punctuation removal\n","    text = re.sub(r'[^\\w\\s]', '', text)\n","\n","    # Tokenization\n","    tokenizer = TweetTokenizer(\n","        preserve_case=False,\n","        strip_handles=True,\n","        reduce_len=True\n","    )\n","    text_tokens = tokenizer.tokenize(text)\n","\n","    return text_tokens"],"metadata":{"id":"85fRweb4AbW1","executionInfo":{"status":"ok","timestamp":1730639843410,"user_tz":-420,"elapsed":5,"user":{"displayName":"Hanh Mai","userId":"06375785904431938117"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["def get_freqs(df):\n","    freqs = defaultdict(lambda: 0)\n","    for idx, row in df.iterrows():\n","        tweet = row['tweet']\n","        label = row['label']\n","\n","        tokens = text_normalize(tweet)\n","        for token in tokens:\n","            pair = (token, label)\n","            freqs[pair] += 1\n","\n","    return freqs"],"metadata":{"id":"vvjd_PHAAbZV","executionInfo":{"status":"ok","timestamp":1730639843410,"user_tz":-420,"elapsed":4,"user":{"displayName":"Hanh Mai","userId":"06375785904431938117"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["def get_feature(text, freqs):\n","    tokens = text_normalize(text)\n","    X = np.zeros(3)\n","    X[0] = 1  # Bias term\n","\n","    for token in tokens:\n","        X[1] += freqs[(token, 0)]  # Count for label 0\n","        X[2] += freqs[(token, 1)]  # Count for label 1\n","\n","    return X"],"metadata":{"id":"KqzwnNOWAbcN","executionInfo":{"status":"ok","timestamp":1730639843411,"user_tz":-420,"elapsed":5,"user":{"displayName":"Hanh Mai","userId":"06375785904431938117"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["# Initialize feature and label lists\n","X = []\n","y = []\n","\n","# Generate frequency dictionary\n","freqs = get_freqs(df)\n","\n","# Populate feature and label arrays\n","for idx, row in df.iterrows():\n","    tweet = row['tweet']\n","    label = row['label']\n","    X_i = get_feature(tweet, freqs)\n","\n","    X.append(X_i)\n","    y.append(label)\n","\n","# Convert lists to numpy arrays\n","X = np.array(X)\n","y = np.array(y)"],"metadata":{"id":"a35ENQvNAbe1","executionInfo":{"status":"ok","timestamp":1730639847738,"user_tz":-420,"elapsed":4331,"user":{"displayName":"Hanh Mai","userId":"06375785904431938117"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["# Set train, validation, and test split parameters\n","val_size = 0.2\n","test_size = 0.125\n","random_state = 2\n","is_shuffle = True\n","\n","# First split: training and validation sets\n","X_train, X_val, y_train, y_val = train_test_split(\n","    X, y,\n","    test_size=val_size,\n","    random_state=random_state,\n","    shuffle=is_shuffle\n",")\n","\n","# Second split: training and test sets\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X_train, y_train,\n","    test_size=test_size,\n","    random_state=random_state,\n","    shuffle=is_shuffle\n",")"],"metadata":{"id":"SvS_Wp4RKnPP","executionInfo":{"status":"ok","timestamp":1730639847739,"user_tz":-420,"elapsed":6,"user":{"displayName":"Hanh Mai","userId":"06375785904431938117"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["# Normalize the data\n","normalizer = StandardScaler()\n","X_train[:, 1:] = normalizer.fit_transform(X_train[:, 1:])\n","X_val[:, 1:] = normalizer.transform(X_val[:, 1:])\n","X_test[:, 1:] = normalizer.transform(X_test[:, 1:])"],"metadata":{"id":"2KNCz_dgKnTt","executionInfo":{"status":"ok","timestamp":1730639847739,"user_tz":-420,"elapsed":6,"user":{"displayName":"Hanh Mai","userId":"06375785904431938117"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["def sigmoid(z):\n","    return 1 / (1 + np.exp(-z))\n","\n","def compute_loss(y_hat, y):\n","    y_hat = np.clip(y_hat, 1e-7, 1 - 1e-7)\n","    return (-y * np.log(y_hat) - (1 - y) * np.log(1 - y_hat)).mean()\n","\n","def predict(X, theta):\n","    dot_product = np.dot(X, theta)\n","    y_hat = sigmoid(dot_product)\n","    return y_hat\n","\n","def compute_gradient(X, y, y_hat):\n","    return np.dot(X.T, (y_hat - y)) / y.size\n","\n","def update_theta(theta, gradient, lr):\n","    return theta - lr * gradient\n","\n","def compute_accuracy(X, y, theta):\n","    y_hat = predict(X, theta).round()\n","    acc = (y_hat == y).mean()\n","    return acc"],"metadata":{"id":"7ayKkyicK4Fr","executionInfo":{"status":"ok","timestamp":1730639847739,"user_tz":-420,"elapsed":6,"user":{"displayName":"Hanh Mai","userId":"06375785904431938117"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["# Initialize hyperparameters and weights\n","lr = 0.01\n","epochs = 200\n","batch_size = 128\n","np.random.seed(random_state)\n","theta = np.random.uniform(size=X_train.shape[1])"],"metadata":{"id":"bIZnf2R4K4M1","executionInfo":{"status":"ok","timestamp":1730639847740,"user_tz":-420,"elapsed":6,"user":{"displayName":"Hanh Mai","userId":"06375785904431938117"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["# Evaluate on validation and test sets\n","val_set_acc = compute_accuracy(X_val, y_val, theta)\n","test_set_acc = compute_accuracy(X_test, y_test, theta)\n","print('Evaluation on validation and test set:')\n","print(f'Validation Set Accuracy: {val_set_acc}')\n","print(f'Test Set Accuracy: {test_set_acc}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0XQ0GHgRK9M3","executionInfo":{"status":"ok","timestamp":1730639847740,"user_tz":-420,"elapsed":6,"user":{"displayName":"Hanh Mai","userId":"06375785904431938117"}},"outputId":"bca0f71b-6817-40b5-dfe1-d7971b1faea6"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Evaluation on validation and test set:\n","Validation Set Accuracy: 0.48674242424242425\n","Test Set Accuracy: 0.48737373737373735\n"]}]}]}