{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "poFbgmSsT7dt",
        "outputId": "1119cd59-dae3-4299-861f-09e5574a7f75"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: gdown--id: command not found\n"
          ]
        }
      ],
      "source": [
        "!gdown--id 1qiUDDoYyRLBiKOoYWdFl_5WByHE8Cugu"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "_x8RW8EUUF5A"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random_state = 59\n",
        "np.random.seed(random_state)\n",
        "torch.manual_seed(random_state)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(random_state)"
      ],
      "metadata": {
        "id": "hXGEuIF5UF7-"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "z5r9NJFwUF-0"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = '/content/Auto_MPG_data.csv'\n",
        "dataset = pd.read_csv(dataset_path)"
      ],
      "metadata": {
        "id": "9m8L5TlDUGBx"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing data\n",
        "X = dataset.drop(columns='MPG').values\n",
        "y = dataset['MPG'].values"
      ],
      "metadata": {
        "id": "mZn22ABYUGEX"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_size = 0.2\n",
        "test_size = 0.125\n",
        "is_shuffle = True\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, y,\n",
        "    test_size=val_size,\n",
        "    random_state=random_state,\n",
        "    shuffle=is_shuffle\n",
        ")\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_train, y_train,\n",
        "    test_size=test_size,\n",
        "    random_state=random_state,\n",
        "    shuffle=is_shuffle\n",
        ")"
      ],
      "metadata": {
        "id": "MhAQJnJXUGHc"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "normalizer = StandardScaler()\n",
        "X_train = normalizer.fit_transform(X_train)\n",
        "X_val = normalizer.transform(X_val)\n",
        "X_test = normalizer.transform(X_test)\n",
        "\n",
        "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_val = torch.tensor(X_val, dtype=torch.float32)\n",
        "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
        "y_val = torch.tensor(y_val, dtype=torch.float32)\n",
        "y_test = torch.tensor(y_test, dtype=torch.float32)"
      ],
      "metadata": {
        "id": "eRb4Un26UGKq"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]"
      ],
      "metadata": {
        "id": "C0kUCazQUGNu"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "train_dataset = CustomDataset(X_train, y_train)\n",
        "val_dataset = CustomDataset(X_val, y_val)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "TmNVRw1kUGQ1"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_dims,hidden_dims,output_dims):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(input_dims,hidden_dims)\n",
        "        self.linear2 = nn.Linear(hidden_dims,hidden_dims)\n",
        "        self.output = nn.Linear(hidden_dims,output_dims)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.linear2(x)\n",
        "        x = F.relu(x)\n",
        "        out = self.output(x)\n",
        "        return out.squeeze(1)\n",
        "\n",
        "# class MLP(nn.Module):\n",
        "#     def __init__(self, input_dims, hidden_dims, output_dims):\n",
        "#         super().__init__()\n",
        "#         self.output = nn.Linear(input_dims, output_dims)  # Chỉ sử dụng 1 lớp Linear\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         out = self.output(x)  # Không sử dụng hàm kích hoạt\n",
        "#         return out.squeeze(1)\n",
        "\n",
        "# class MLP(nn.Module):\n",
        "#     def __init__(self, input_dims, hidden_dims, output_dims):\n",
        "#         super().__init__()\n",
        "#         self.linear1 = nn.Linear(input_dims, hidden_dims)\n",
        "#         self.linear2 = nn.Linear(hidden_dims, hidden_dims)\n",
        "#         self.output = nn.Linear(hidden_dims, output_dims)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self.linear1(x)\n",
        "#         x = torch.sigmoid(x)  # Dùng hàm Sigmoid\n",
        "#         x = self.linear2(x)\n",
        "#         x = torch.sigmoid(x)  # Dùng hàm Sigmoid\n",
        "#         out = self.output(x)\n",
        "#         return out.squeeze(1)\n",
        "\n",
        "# class MLP(nn.Module):\n",
        "#     def __init__(self, input_dims, hidden_dims, output_dims):\n",
        "#         super().__init__()\n",
        "#         self.linear1 = nn.Linear(input_dims, hidden_dims)\n",
        "#         self.linear2 = nn.Linear(hidden_dims, hidden_dims)\n",
        "#         self.output = nn.Linear(hidden_dims, output_dims)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self.linear1(x)\n",
        "#         x = torch.tanh(x)  # Dùng hàm Tanh\n",
        "#         x = self.linear2(x)\n",
        "#         x = torch.tanh(x)  # Dùng hàm Tanh\n",
        "#         out = self.output(x)\n",
        "#         return out.squeeze(1)"
      ],
      "metadata": {
        "id": "idpUWxhxUGUL"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_dims = X_train.shape[1]\n",
        "output_dims = 1\n",
        "hidden_dims = 64\n",
        "\n",
        "model = MLP(input_dims=input_dims,\n",
        "            hidden_dims=hidden_dims,\n",
        "            output_dims=output_dims).to(device)"
      ],
      "metadata": {
        "id": "PBGqOQeBUGXT"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 1e-2\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=lr)"
      ],
      "metadata": {
        "id": "piZYrUt0UGad"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def r_squared(y_true, y_pred):\n",
        "  y_true = torch.Tensor(y_true).to(device)\n",
        "  y_pred = torch.Tensor(y_pred).to(device)\n",
        "  mean_true = torch.mean(y_true)\n",
        "  ss_tot = torch.sum((y_true - mean_true) ** 2)\n",
        "  ss_res = torch.sum((y_true - y_pred) ** 2)\n",
        "  r2 = 1 - (ss_res / ss_tot)\n",
        "  return r2"
      ],
      "metadata": {
        "id": "wH9iFy75XYhi"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 100\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "train_r2 = []\n",
        "val_r2 = []\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    # Initialize training and validation metrics\n",
        "    train_loss = 0.0\n",
        "    train_target, train_predict = [], []\n",
        "    val_target, val_predict = [], []\n",
        "\n",
        "    # Training phase\n",
        "    model.train()\n",
        "    for X_samples, y_samples in train_loader:\n",
        "        X_samples, y_samples = X_samples.to(device), y_samples.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(X_samples)\n",
        "\n",
        "        train_predict += outputs.tolist()\n",
        "        train_target += y_samples.tolist()\n",
        "\n",
        "        loss = criterion(outputs, y_samples)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    train_loss /= len(train_loader)\n",
        "    train_losses.append(train_loss)\n",
        "    train_r2.append(r_squared(train_target, train_predict))\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for X_samples, y_samples in val_loader:\n",
        "            X_samples, y_samples = X_samples.to(device), y_samples.to(device)\n",
        "\n",
        "            outputs = model(X_samples)\n",
        "            val_predict += outputs.tolist()\n",
        "            val_target += y_samples.tolist()\n",
        "\n",
        "            loss = criterion(outputs, y_samples)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "    val_loss /= len(val_loader)\n",
        "    val_losses.append(val_loss)\n",
        "    val_r2.append(r_squared(val_target, val_predict))\n",
        "\n",
        "    # Print epoch summary\n",
        "    print(f\"\\nEPOCH {epoch + 1}: Training loss: {train_loss:.3f} Validation loss: {val_loss:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kXKOMCrCXYns",
        "outputId": "f9dc8f7c-f89e-4bf1-f915-e46daebf6968"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "EPOCH 1: Training loss: 241.367 Validation loss: 16.744\n",
            "\n",
            "EPOCH 2: Training loss: 25.802 Validation loss: 10.408\n",
            "\n",
            "EPOCH 3: Training loss: 17.005 Validation loss: 8.828\n",
            "\n",
            "EPOCH 4: Training loss: 12.898 Validation loss: 7.989\n",
            "\n",
            "EPOCH 5: Training loss: 12.065 Validation loss: 7.318\n",
            "\n",
            "EPOCH 6: Training loss: 10.612 Validation loss: 6.559\n",
            "\n",
            "EPOCH 7: Training loss: 9.599 Validation loss: 6.742\n",
            "\n",
            "EPOCH 8: Training loss: 9.545 Validation loss: 8.186\n",
            "\n",
            "EPOCH 9: Training loss: 9.627 Validation loss: 11.149\n",
            "\n",
            "EPOCH 10: Training loss: 10.421 Validation loss: 12.589\n",
            "\n",
            "EPOCH 11: Training loss: 10.581 Validation loss: 6.085\n",
            "\n",
            "EPOCH 12: Training loss: 8.204 Validation loss: 5.374\n",
            "\n",
            "EPOCH 13: Training loss: 8.724 Validation loss: 6.749\n",
            "\n",
            "EPOCH 14: Training loss: 8.068 Validation loss: 5.731\n",
            "\n",
            "EPOCH 15: Training loss: 7.931 Validation loss: 6.713\n",
            "\n",
            "EPOCH 16: Training loss: 8.309 Validation loss: 6.800\n",
            "\n",
            "EPOCH 17: Training loss: 7.815 Validation loss: 6.303\n",
            "\n",
            "EPOCH 18: Training loss: 7.992 Validation loss: 6.095\n",
            "\n",
            "EPOCH 19: Training loss: 8.061 Validation loss: 5.772\n",
            "\n",
            "EPOCH 20: Training loss: 7.382 Validation loss: 5.462\n",
            "\n",
            "EPOCH 21: Training loss: 7.250 Validation loss: 6.422\n",
            "\n",
            "EPOCH 22: Training loss: 7.591 Validation loss: 7.302\n",
            "\n",
            "EPOCH 23: Training loss: 8.226 Validation loss: 10.787\n",
            "\n",
            "EPOCH 24: Training loss: 7.973 Validation loss: 10.639\n",
            "\n",
            "EPOCH 25: Training loss: 7.888 Validation loss: 7.459\n",
            "\n",
            "EPOCH 26: Training loss: 7.659 Validation loss: 8.165\n",
            "\n",
            "EPOCH 27: Training loss: 7.137 Validation loss: 6.086\n",
            "\n",
            "EPOCH 28: Training loss: 6.890 Validation loss: 4.627\n",
            "\n",
            "EPOCH 29: Training loss: 6.258 Validation loss: 8.012\n",
            "\n",
            "EPOCH 30: Training loss: 7.585 Validation loss: 4.976\n",
            "\n",
            "EPOCH 31: Training loss: 6.797 Validation loss: 5.659\n",
            "\n",
            "EPOCH 32: Training loss: 6.483 Validation loss: 5.494\n",
            "\n",
            "EPOCH 33: Training loss: 7.160 Validation loss: 4.980\n",
            "\n",
            "EPOCH 34: Training loss: 6.735 Validation loss: 5.562\n",
            "\n",
            "EPOCH 35: Training loss: 6.635 Validation loss: 5.817\n",
            "\n",
            "EPOCH 36: Training loss: 6.514 Validation loss: 7.111\n",
            "\n",
            "EPOCH 37: Training loss: 6.783 Validation loss: 6.540\n",
            "\n",
            "EPOCH 38: Training loss: 6.381 Validation loss: 6.996\n",
            "\n",
            "EPOCH 39: Training loss: 6.705 Validation loss: 8.441\n",
            "\n",
            "EPOCH 40: Training loss: 6.391 Validation loss: 6.462\n",
            "\n",
            "EPOCH 41: Training loss: 6.446 Validation loss: 7.248\n",
            "\n",
            "EPOCH 42: Training loss: 6.036 Validation loss: 4.792\n",
            "\n",
            "EPOCH 43: Training loss: 7.205 Validation loss: 9.273\n",
            "\n",
            "EPOCH 44: Training loss: 6.420 Validation loss: 9.756\n",
            "\n",
            "EPOCH 45: Training loss: 6.502 Validation loss: 5.212\n",
            "\n",
            "EPOCH 46: Training loss: 6.290 Validation loss: 6.014\n",
            "\n",
            "EPOCH 47: Training loss: 6.520 Validation loss: 6.545\n",
            "\n",
            "EPOCH 48: Training loss: 5.925 Validation loss: 7.348\n",
            "\n",
            "EPOCH 49: Training loss: 6.302 Validation loss: 8.034\n",
            "\n",
            "EPOCH 50: Training loss: 6.054 Validation loss: 6.291\n",
            "\n",
            "EPOCH 51: Training loss: 5.725 Validation loss: 6.172\n",
            "\n",
            "EPOCH 52: Training loss: 6.359 Validation loss: 5.266\n",
            "\n",
            "EPOCH 53: Training loss: 6.484 Validation loss: 5.407\n",
            "\n",
            "EPOCH 54: Training loss: 6.310 Validation loss: 5.479\n",
            "\n",
            "EPOCH 55: Training loss: 5.707 Validation loss: 5.466\n",
            "\n",
            "EPOCH 56: Training loss: 5.896 Validation loss: 6.567\n",
            "\n",
            "EPOCH 57: Training loss: 6.222 Validation loss: 5.896\n",
            "\n",
            "EPOCH 58: Training loss: 5.692 Validation loss: 5.570\n",
            "\n",
            "EPOCH 59: Training loss: 5.499 Validation loss: 6.238\n",
            "\n",
            "EPOCH 60: Training loss: 5.388 Validation loss: 5.107\n",
            "\n",
            "EPOCH 61: Training loss: 5.248 Validation loss: 5.777\n",
            "\n",
            "EPOCH 62: Training loss: 6.086 Validation loss: 5.257\n",
            "\n",
            "EPOCH 63: Training loss: 5.305 Validation loss: 5.743\n",
            "\n",
            "EPOCH 64: Training loss: 5.250 Validation loss: 5.697\n",
            "\n",
            "EPOCH 65: Training loss: 5.727 Validation loss: 6.221\n",
            "\n",
            "EPOCH 66: Training loss: 5.243 Validation loss: 5.986\n",
            "\n",
            "EPOCH 67: Training loss: 5.316 Validation loss: 5.275\n",
            "\n",
            "EPOCH 68: Training loss: 5.562 Validation loss: 5.670\n",
            "\n",
            "EPOCH 69: Training loss: 6.100 Validation loss: 6.195\n",
            "\n",
            "EPOCH 70: Training loss: 5.377 Validation loss: 5.671\n",
            "\n",
            "EPOCH 71: Training loss: 5.128 Validation loss: 5.187\n",
            "\n",
            "EPOCH 72: Training loss: 5.258 Validation loss: 5.934\n",
            "\n",
            "EPOCH 73: Training loss: 5.104 Validation loss: 6.466\n",
            "\n",
            "EPOCH 74: Training loss: 5.096 Validation loss: 5.614\n",
            "\n",
            "EPOCH 75: Training loss: 4.985 Validation loss: 7.470\n",
            "\n",
            "EPOCH 76: Training loss: 5.683 Validation loss: 7.358\n",
            "\n",
            "EPOCH 77: Training loss: 5.975 Validation loss: 6.229\n",
            "\n",
            "EPOCH 78: Training loss: 5.481 Validation loss: 5.142\n",
            "\n",
            "EPOCH 79: Training loss: 5.387 Validation loss: 5.927\n",
            "\n",
            "EPOCH 80: Training loss: 5.142 Validation loss: 6.230\n",
            "\n",
            "EPOCH 81: Training loss: 5.382 Validation loss: 5.407\n",
            "\n",
            "EPOCH 82: Training loss: 4.667 Validation loss: 5.287\n",
            "\n",
            "EPOCH 83: Training loss: 5.038 Validation loss: 6.840\n",
            "\n",
            "EPOCH 84: Training loss: 4.770 Validation loss: 5.772\n",
            "\n",
            "EPOCH 85: Training loss: 4.651 Validation loss: 5.814\n",
            "\n",
            "EPOCH 86: Training loss: 4.551 Validation loss: 7.225\n",
            "\n",
            "EPOCH 87: Training loss: 4.777 Validation loss: 5.901\n",
            "\n",
            "EPOCH 88: Training loss: 4.768 Validation loss: 5.766\n",
            "\n",
            "EPOCH 89: Training loss: 4.376 Validation loss: 5.947\n",
            "\n",
            "EPOCH 90: Training loss: 4.901 Validation loss: 5.733\n",
            "\n",
            "EPOCH 91: Training loss: 5.195 Validation loss: 6.175\n",
            "\n",
            "EPOCH 92: Training loss: 4.624 Validation loss: 5.574\n",
            "\n",
            "EPOCH 93: Training loss: 4.535 Validation loss: 5.527\n",
            "\n",
            "EPOCH 94: Training loss: 5.158 Validation loss: 5.980\n",
            "\n",
            "EPOCH 95: Training loss: 5.022 Validation loss: 7.888\n",
            "\n",
            "EPOCH 96: Training loss: 5.298 Validation loss: 5.606\n",
            "\n",
            "EPOCH 97: Training loss: 4.863 Validation loss: 8.047\n",
            "\n",
            "EPOCH 98: Training loss: 5.057 Validation loss: 6.092\n",
            "\n",
            "EPOCH 99: Training loss: 4.539 Validation loss: 5.775\n",
            "\n",
            "EPOCH 100: Training loss: 4.707 Validation loss: 5.689\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "  # Move X_test to the same device as the model\n",
        "  X_test = X_test.to(device)  # Add this line to move X_test to the GPU\n",
        "  y_hat = model(X_test)\n",
        "  test_set_r2 = r_squared(y_hat, y_test)\n",
        "  print('Evaluation on test set:')\n",
        "  print(f'R2: {test_set_r2}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r0DdBnXjXoP-",
        "outputId": "8c27c943-1f09-4beb-ee38-c8c17e2361a4"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation on test set:\n",
            "R2: 0.8503546118736267\n"
          ]
        }
      ]
    }
  ]
}