{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNXOFtezRiSn8n8wU0apNSF"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Bài 1: Gradient Descent"],"metadata":{"id":"LZZQ4QkD45JO"}},{"cell_type":"code","execution_count":5,"metadata":{"id":"e6BPFHjy1CdE","executionInfo":{"status":"ok","timestamp":1732631899194,"user_tz":-420,"elapsed":527,"user":{"displayName":"Hanh Mai","userId":"06375785904431938117"}}},"outputs":[],"source":["import numpy as np\n","\n","def df_w(W):\n","    \"\"\"\n","    Compute the gradents of dw1 and dw2.\n","    Arguments:\n","    W -- np.array [w1, w2]\n","    Returns:\n","    dw -- np.array [dw1, dw2], array containing the partial derivatives\n","    \"\"\"\n","    w1, w2 = W\n","\n","    dw1 = 2 * w1\n","    dw2 = 2 * w2\n","\n","    dw = np.array([dw1, dw2])\n","    return dw"]},{"cell_type":"code","source":["def sgd(W, dw, lr):\n","    \"\"\"\n","    Perform a single gradient descent step to update w1 and w2.\n","    Arguments:\n","    W -- np.array [w1, w2]\n","    dw -- np.array [dw1, dw2], array of gradients\n","    lr -- float, learning rate\n","    Returns:\n","    W -- np.array [w1, w2] after update\n","    \"\"\"\n","    W = W - lr * dw\n","    return W"],"metadata":{"id":"GmeT3jCa4iaT","executionInfo":{"status":"ok","timestamp":1732631899194,"user_tz":-420,"elapsed":7,"user":{"displayName":"Hanh Mai","userId":"06375785904431938117"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["def train_p1(optimizer, lr, epochs):\n","    \"\"\"\n","    Train to find the minimum using Gradient Descent.\n","    Arguments:\n","    optimizer -- function for optimization (sgd)\n","    lr -- float, learning rate\n","    epochs -- int, number of epochs\n","    Returns:\n","    results -- list of [w1, w2] values after each epoch\n","    \"\"\"\n","    # Initial point\n","    W = np.array([-5.0, -2.0], dtype=np.float32)\n","    # List of results\n","    results = [W]\n","\n","    # Loop through epochs\n","    for epoch in range(epochs):\n","        # Compute gradients\n","        dw = df_w(W)\n","        # Update weights using the optimizer\n","        W = optimizer(W, dw, lr)\n","        # Append the new weights to results\n","        results.append(W)\n","\n","    return results"],"metadata":{"id":"NDFpuEOo4ifP","executionInfo":{"status":"ok","timestamp":1732631899195,"user_tz":-420,"elapsed":8,"user":{"displayName":"Hanh Mai","userId":"06375785904431938117"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["# # Parameters\n","# learning_rate = 0.1\n","# epochs = 2\n","\n","# # Run training\n","# results = train_p1(sgd, learning_rate, epochs)\n","\n","# # Print results\n","# for i, W in enumerate(results):\n","#     print(f\"Epoch {i}: w1 = {W[0]}, w2 = {W[1]}\")"],"metadata":{"id":"y5TDG1wE4ikv","executionInfo":{"status":"ok","timestamp":1732631899195,"user_tz":-420,"elapsed":8,"user":{"displayName":"Hanh Mai","userId":"06375785904431938117"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["learning_rate = 0.1\n","epochs = 30\n","results = train_p1(sgd, learning_rate, epochs)\n","\n","for i, W in enumerate(results):\n","    print(f\"Epoch {i}: w1 = {W[0]}, w2 = {W[1]}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_yVeVUPb51T0","executionInfo":{"status":"ok","timestamp":1732631899195,"user_tz":-420,"elapsed":7,"user":{"displayName":"Hanh Mai","userId":"06375785904431938117"}},"outputId":"51099ebf-67e4-4dff-8b7b-51a9c421d49f"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 0: w1 = -5.0, w2 = -2.0\n","Epoch 1: w1 = -4.0, w2 = -1.6\n","Epoch 2: w1 = -3.2, w2 = -1.28\n","Epoch 3: w1 = -2.56, w2 = -1.024\n","Epoch 4: w1 = -2.048, w2 = -0.8192\n","Epoch 5: w1 = -1.6384, w2 = -0.65536\n","Epoch 6: w1 = -1.31072, w2 = -0.5242880000000001\n","Epoch 7: w1 = -1.0485760000000002, w2 = -0.4194304000000001\n","Epoch 8: w1 = -0.8388608000000002, w2 = -0.33554432000000006\n","Epoch 9: w1 = -0.6710886400000001, w2 = -0.26843545600000007\n","Epoch 10: w1 = -0.5368709120000001, w2 = -0.21474836480000006\n","Epoch 11: w1 = -0.4294967296000001, w2 = -0.17179869184000005\n","Epoch 12: w1 = -0.3435973836800001, w2 = -0.13743895347200002\n","Epoch 13: w1 = -0.27487790694400005, w2 = -0.10995116277760002\n","Epoch 14: w1 = -0.21990232555520003, w2 = -0.08796093022208001\n","Epoch 15: w1 = -0.17592186044416003, w2 = -0.070368744177664\n","Epoch 16: w1 = -0.140737488355328, w2 = -0.056294995342131206\n","Epoch 17: w1 = -0.11258999068426241, w2 = -0.04503599627370496\n","Epoch 18: w1 = -0.09007199254740993, w2 = -0.03602879701896397\n","Epoch 19: w1 = -0.07205759403792794, w2 = -0.028823037615171177\n","Epoch 20: w1 = -0.057646075230342354, w2 = -0.02305843009213694\n","Epoch 21: w1 = -0.04611686018427388, w2 = -0.018446744073709553\n","Epoch 22: w1 = -0.03689348814741911, w2 = -0.014757395258967642\n","Epoch 23: w1 = -0.029514790517935284, w2 = -0.011805916207174114\n","Epoch 24: w1 = -0.02361183241434823, w2 = -0.009444732965739291\n","Epoch 25: w1 = -0.018889465931478583, w2 = -0.0075557863725914335\n","Epoch 26: w1 = -0.015111572745182867, w2 = -0.006044629098073147\n","Epoch 27: w1 = -0.012089258196146294, w2 = -0.004835703278458518\n","Epoch 28: w1 = -0.009671406556917036, w2 = -0.003868562622766814\n","Epoch 29: w1 = -0.007737125245533628, w2 = -0.0030948500982134514\n","Epoch 30: w1 = -0.006189700196426903, w2 = -0.002475880078570761\n"]}]},{"cell_type":"markdown","source":["# Bài 2: Gradient Descent + Momentum\n"],"metadata":{"id":"ihGK0KM-6Pu6"}},{"cell_type":"code","source":["def df_w(W):\n","    \"\"\"\n","    Compute the gradients of dw1 and dw2 for the function.\n","    Arguments:\n","    W -- np.array [w1, w2]\n","    Returns:\n","    dw -- np.array [dw1, dw2]\n","    \"\"\"\n","    w1, w2 = W\n","    dw1 = 2 * w1\n","    dw2 = 2 * w2\n","    return np.array([dw1, dw2])"],"metadata":{"id":"Qa5Y-xqL7QNg","executionInfo":{"status":"ok","timestamp":1732636381990,"user_tz":-420,"elapsed":1234,"user":{"displayName":"Hanh Mai","userId":"06375785904431938117"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["def sgd_momentum(W, V, dw, lr, beta):\n","    \"\"\"\n","    Perform the gradient descent update with momentum.\n","    Arguments:\n","    W -- np.array [w1, w2]\n","    V -- np.array [V1, V2], velocity vector\n","    dw -- np.array [dw1, dw2], gradients\n","    lr -- float, learning rate\n","    beta -- float, momentum factor\n","    Returns:\n","    W -- np.array [w1, w2] after update\n","    V -- np.array [V1, V2], updated velocity\n","    \"\"\"\n","    # Update velocity (equation 2.1)\n","    V = beta * V + (1 - beta) * dw\n","    # Update weights (equation 2.2)\n","    W = W - lr * V\n","    return W, V"],"metadata":{"id":"ExPof7u76Vgy","executionInfo":{"status":"ok","timestamp":1732636383233,"user_tz":-420,"elapsed":1,"user":{"displayName":"Hanh Mai","userId":"06375785904431938117"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["# Initial parameters\n","W = np.array([-5.0, -2.0], dtype=np.float32)  # Initial weights [w1, w2]\n","V = np.array([0.0, 0.0], dtype=np.float32)    # Initial velocity [v1, v2]\n","learning_rate = 0.1                           # Learning rate\n","beta = 0.9                                    # Momentum factor\n","epochs = 30 # 2                                   # Number of epochs"],"metadata":{"id":"VEkNon3-6V65","executionInfo":{"status":"ok","timestamp":1732636445045,"user_tz":-420,"elapsed":349,"user":{"displayName":"Hanh Mai","userId":"06375785904431938117"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["# Perform Gradient Descent + Momentum for 2 epochs\n","for epoch in range(1, epochs + 1):\n","    print(f\"Epoch {epoch}:\")\n","\n","    # Step 1: Compute the gradients\n","    dw = df_w(W)\n","    print(f\"  Gradient at W: dw1 = {dw[0]}, dw2 = {dw[1]}\")\n","\n","    # Step 2 & 3: Update velocity and weights using Gradient Descent + Momentum\n","    W, V = sgd_momentum(W, V, dw, learning_rate, beta)\n","    print(f\"  Updated W: w1 = {W[0]}, w2 = {W[1]}\")\n","    print(f\"  Updated Velocity: V1 = {V[0]}, V2 = {V[1]}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mCtu4TIp6V9c","executionInfo":{"status":"ok","timestamp":1732636447212,"user_tz":-420,"elapsed":3,"user":{"displayName":"Hanh Mai","userId":"06375785904431938117"}},"outputId":"fdbf88c5-40f5-4302-8490-5fcd1f749b89"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1:\n","  Gradient at W: dw1 = -10.0, dw2 = -4.0\n","  Updated W: w1 = -4.9, w2 = -1.96\n","  Updated Velocity: V1 = -0.9999999999999998, V2 = -0.3999999999999999\n","Epoch 2:\n","  Gradient at W: dw1 = -9.8, dw2 = -3.92\n","  Updated W: w1 = -4.712000000000001, w2 = -1.8848\n","  Updated Velocity: V1 = -1.8799999999999997, V2 = -0.7519999999999998\n","Epoch 3:\n","  Gradient at W: dw1 = -9.424000000000001, dw2 = -3.7696\n","  Updated W: w1 = -4.4485600000000005, w2 = -1.7794240000000001\n","  Updated Velocity: V1 = -2.6343999999999994, V2 = -1.0537599999999998\n","Epoch 4:\n","  Gradient at W: dw1 = -8.897120000000001, dw2 = -3.5588480000000002\n","  Updated W: w1 = -4.122492800000001, w2 = -1.6489971200000002\n","  Updated Velocity: V1 = -3.2606719999999996, V2 = -1.3042687999999998\n","Epoch 5:\n","  Gradient at W: dw1 = -8.244985600000001, dw2 = -3.2979942400000004\n","  Updated W: w1 = -3.7465824640000007, w2 = -1.4986329856000002\n","  Updated Velocity: V1 = -3.7591033599999997, V2 = -1.5036413439999998\n","Epoch 6:\n","  Gradient at W: dw1 = -7.4931649280000014, dw2 = -2.9972659712000005\n","  Updated W: w1 = -3.333331512320001, w2 = -1.3333326049280003\n","  Updated Velocity: V1 = -4.1325095168, V2 = -1.65300380672\n","Epoch 7:\n","  Gradient at W: dw1 = -6.666663024640002, dw2 = -2.6666652098560006\n","  Updated W: w1 = -2.894739025561601, w2 = -1.1578956102246403\n","  Updated Velocity: V1 = -4.385924867584, V2 = -1.7543699470336\n","Epoch 8:\n","  Gradient at W: dw1 = -5.789478051123202, dw2 = -2.3157912204492805\n","  Updated W: w1 = -2.4421110069678087, w2 = -0.9768444027871235\n","  Updated Velocity: V1 = -4.52628018593792, V2 = -1.810512074375168\n","Epoch 9:\n","  Gradient at W: dw1 = -4.884222013935617, dw2 = -1.953688805574247\n","  Updated W: w1 = -1.9859035700940397, w2 = -0.7943614280376159\n","  Updated Velocity: V1 = -4.562074368737689, V2 = -1.8248297474950759\n","Epoch 10:\n","  Gradient at W: dw1 = -3.9718071401880795, dw2 = -1.5887228560752318\n","  Updated W: w1 = -1.5355988055057668, w2 = -0.6142395222023067\n","  Updated Velocity: V1 = -4.503047645882728, V2 = -1.8012190583530914\n","Epoch 11:\n","  Gradient at W: dw1 = -3.0711976110115335, dw2 = -1.2284790444046134\n","  Updated W: w1 = -1.099612541266206, w2 = -0.43984501650648233\n","  Updated Velocity: V1 = -4.359862642395608, V2 = -1.7439450569582435\n","Epoch 12:\n","  Gradient at W: dw1 = -2.199225082532412, dw2 = -0.8796900330129647\n","  Updated W: w1 = -0.685232652625277, w2 = -0.2740930610501108\n","  Updated Velocity: V1 = -4.1437988864092885, V2 = -1.6575195545637158\n","Epoch 13:\n","  Gradient at W: dw1 = -1.370465305250554, dw2 = -0.5481861221002216\n","  Updated W: w1 = -0.2985860997959355, w2 = -0.11943443991837413\n","  Updated Velocity: V1 = -3.866465528293415, V2 = -1.5465862113173663\n","Epoch 14:\n","  Gradient at W: dw1 = -0.597172199591871, dw2 = -0.23886887983674826\n","  Updated W: w1 = 0.05536751974639059, w2 = 0.022147007898556337\n","  Updated Velocity: V1 = -3.5395361954232607, V2 = -1.4158144781693045\n","Epoch 15:\n","  Gradient at W: dw1 = 0.11073503949278118, dw2 = 0.044294015797112674\n","  Updated W: w1 = 0.3728184269395563, w2 = 0.14912737077582264\n","  Updated Velocity: V1 = -3.1745090719316567, V2 = -1.2698036287726628\n","Epoch 16:\n","  Gradient at W: dw1 = 0.7456368538791126, dw2 = 0.2982547415516453\n","  Updated W: w1 = 0.6510678748746144, w2 = 0.26042714994984584\n","  Updated Velocity: V1 = -2.78249447935058, V2 = -1.112997791740232\n","Epoch 17:\n","  Gradient at W: dw1 = 1.3021357497492287, dw2 = 0.5208542998996917\n","  Updated W: w1 = 0.8884710205186743, w2 = 0.3553884082074698\n","  Updated Velocity: V1 = -2.374031456440599, V2 = -0.9496125825762398\n","Epoch 18:\n","  Gradient at W: dw1 = 1.7769420410373487, dw2 = 0.7107768164149396\n","  Updated W: w1 = 1.0843644311879548, w2 = 0.433745772475182\n","  Updated Velocity: V1 = -1.9589341066928043, V2 = -0.7835736426771219\n","Epoch 19:\n","  Gradient at W: dw1 = 2.1687288623759096, dw2 = 0.867491544950364\n","  Updated W: w1 = 1.2389812121665482, w2 = 0.49559248486661933\n","  Updated Velocity: V1 = -1.546167809785933, V2 = -0.6184671239143733\n","Epoch 20:\n","  Gradient at W: dw1 = 2.4779624243330964, dw2 = 0.9911849697332387\n","  Updated W: w1 = 1.3533566908039512, w2 = 0.5413426763215805\n","  Updated Velocity: V1 = -1.1437547863740303, V2 = -0.4575019145496121\n","Epoch 21:\n","  Gradient at W: dw1 = 2.7067133816079023, dw2 = 1.082685352643161\n","  Updated W: w1 = 1.429227487761535, w2 = 0.571690995104614\n","  Updated Velocity: V1 = -0.7587079695758372, V2 = -0.30348318783033484\n","Epoch 22:\n","  Gradient at W: dw1 = 2.85845497552307, dw2 = 1.143381990209228\n","  Updated W: w1 = 1.4689266552681297, w2 = 0.5875706621072518\n","  Updated Velocity: V1 = -0.39699167506594657, V2 = -0.15879667002637862\n","Epoch 23:\n","  Gradient at W: dw1 = 2.9378533105362594, dw2 = 1.1751413242145037\n","  Updated W: w1 = 1.4752773729187023, w2 = 0.5901109491674809\n","  Updated Velocity: V1 = -0.06350717650572607, V2 = -0.025402870602290434\n","Epoch 24:\n","  Gradient at W: dw1 = 2.9505547458374046, dw2 = 1.1802218983349617\n","  Updated W: w1 = 1.4514874713458437, w2 = 0.5805949885383374\n","  Updated Velocity: V1 = 0.2378990157285869, V2 = 0.09515960629143476\n","Epoch 25:\n","  Gradient at W: dw1 = 2.9029749426916873, dw2 = 1.1611899770766747\n","  Updated W: w1 = 1.401046810503354, w2 = 0.5604187242013414\n","  Updated Velocity: V1 = 0.5044066084248969, V2 = 0.20176264336995872\n","Epoch 26:\n","  Gradient at W: dw1 = 2.802093621006708, dw2 = 1.1208374484026828\n","  Updated W: w1 = 1.3276292795350462, w2 = 0.5310517118140183\n","  Updated Velocity: V1 = 0.734175309683078, V2 = 0.2936701238732311\n","Epoch 27:\n","  Gradient at W: dw1 = 2.6552585590700923, dw2 = 1.0621034236280367\n","  Updated W: w1 = 1.2350009160728683, w2 = 0.4940003664291472\n","  Updated Velocity: V1 = 0.9262836346217793, V2 = 0.37051345384871165\n","Epoch 28:\n","  Gradient at W: dw1 = 2.4700018321457367, dw2 = 0.9880007328582944\n","  Updated W: w1 = 1.1269353706354508, w2 = 0.45077414825418016\n","  Updated Velocity: V1 = 1.0806554543741749, V2 = 0.4322621817496699\n","Epoch 29:\n","  Gradient at W: dw1 = 2.2538707412709016, dw2 = 0.9015482965083603\n","  Updated W: w1 = 1.007137672329066, w2 = 0.40285506893162626\n","  Updated Velocity: V1 = 1.1979769830638476, V2 = 0.4791907932255389\n","Epoch 30:\n","  Gradient at W: dw1 = 2.014275344658132, dw2 = 0.8057101378632525\n","  Updated W: w1 = 0.8791769904067384, w2 = 0.35167079616269525\n","  Updated Velocity: V1 = 1.2796068192232761, V2 = 0.5118427276893103\n"]}]},{"cell_type":"markdown","source":["# Bài 3: RMSProp"],"metadata":{"id":"XqAvUYsALi6G"}},{"cell_type":"code","source":["def df_w(W):\n","    \"\"\"\n","    Compute the gradents of dw1 and dw2.\n","    Arguments:\n","    W -- np.array [w1, w2]\n","    Returns:\n","    dw -- np.array [dw1, dw2], array containing the partial derivatives\n","    \"\"\"\n","    w1, w2 = W\n","\n","    dw1 = 2 * w1\n","    dw2 = 2 * w2\n","\n","    dw = np.array([dw1, dw2])\n","    return dw"],"metadata":{"id":"ns5dq8wZ6WGJ","executionInfo":{"status":"ok","timestamp":1732636691675,"user_tz":-420,"elapsed":343,"user":{"displayName":"Hanh Mai","userId":"06375785904431938117"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["# Define the RMSProp update function\n","def rmsprop(W, S, dw, lr, gamma, epsilon):\n","    \"\"\"\n","    Perform the RMSProp update.\n","    Arguments:\n","    W -- np.array [w1, w2]\n","    S -- np.array [s1, s2], the moving average of squared gradients\n","    dw -- np.array [dw1, dw2], gradients\n","    lr -- float, learning rate\n","    gamma -- float, decay rate\n","    epsilon -- float, a small constant to avoid division by zero\n","    Returns:\n","    W -- np.array [w1, w2] after the update\n","    S -- np.array [s1, s2], updated moving average of squared gradients\n","    \"\"\"\n","    # Update S using equation 3.1\n","    S = gamma * S + (1 - gamma) * (dw ** 2)\n","    # Update W using equation 3.2\n","    W = W - (lr * dw) / (np.sqrt(S) + epsilon)\n","    return W, S"],"metadata":{"id":"IGerMZpPMSxk","executionInfo":{"status":"ok","timestamp":1732636692593,"user_tz":-420,"elapsed":3,"user":{"displayName":"Hanh Mai","userId":"06375785904431938117"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["# Training loop for RMSProp\n","def train_p1_rmsprop(lr, gamma, epsilon, epochs):\n","    \"\"\"\n","    Train to find the minimum using RMSProp.\n","    Arguments:\n","    lr -- float, learning rate\n","    gamma -- float, decay rate\n","    epsilon -- float, a small constant to avoid division by zero\n","    epochs -- int, number of epochs\n","    Returns:\n","    results -- list of [w1, w2] values after each epoch\n","    \"\"\"\n","    # Initial point\n","    W = np.array([-5.0, -2.0], dtype=np.float32)  # Initial weights\n","    S = np.array([0.0, 0.0], dtype=np.float32)    # Initial moving average of squared gradients\n","    # List of results to store W at each epoch\n","    results = [W]\n","\n","    # Loop through epochs\n","    for epoch in range(epochs):\n","        # Step 1: Compute gradients\n","        dw = df_w(W)\n","        # Step 2 & 3: Update S and W using RMSProp\n","        W, S = rmsprop(W, S, dw, lr, gamma, epsilon)\n","        # Append the new weights to results\n","        results.append(W)\n","\n","    return results"],"metadata":{"id":"U4cJTM5WMXgn","executionInfo":{"status":"ok","timestamp":1732636694219,"user_tz":-420,"elapsed":369,"user":{"displayName":"Hanh Mai","userId":"06375785904431938117"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["# Parameters for RMSProp\n","learning_rate = 0.1  # Learning rate\n","gamma = 0.9          # Decay rate\n","epsilon = 1e-8       # Small constant to prevent division by zero\n","epochs = 30          # Number of epochs"],"metadata":{"id":"t1h8PDeuMZXd","executionInfo":{"status":"ok","timestamp":1732636695844,"user_tz":-420,"elapsed":336,"user":{"displayName":"Hanh Mai","userId":"06375785904431938117"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["# Perform RMSProp for 2 epochs\n","results = train_p1_rmsprop(learning_rate, gamma, epsilon, epochs)\n","\n","# Display the results\n","for i, W in enumerate(results):\n","    print(f\"Epoch {i}: w1 = {W[0]}, w2 = {W[1]}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"88OCGu0UMbCk","executionInfo":{"status":"ok","timestamp":1732636697180,"user_tz":-420,"elapsed":6,"user":{"displayName":"Hanh Mai","userId":"06375785904431938117"}},"outputId":"40279c57-eb2c-4f53-96a0-97b624359c4f"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 0: w1 = -5.0, w2 = -2.0\n","Epoch 1: w1 = -4.683772234983162, w2 = -1.683772236483162\n","Epoch 2: w1 = -4.461584607692758, w2 = -1.4738753171325136\n","Epoch 3: w1 = -4.27928872720579, w2 = -1.3087178712916374\n","Epoch 4: w1 = -4.120054383810811, w2 = -1.1698398157391754\n","Epoch 5: w1 = -3.9761536215788174, w2 = -1.0489273067370706\n","Epoch 6: w1 = -3.843310019236342, w2 = -0.9414504929904555\n","Epoch 7: w1 = -3.7188778308800394, w2 = -0.8446493453528385\n","Epoch 8: w1 = -3.601093559818063, w2 = -0.7567142198996779\n","Epoch 9: w1 = -3.488717515023634, w2 = -0.6763956470812718\n","Epoch 10: w1 = -3.380842895309756, w2 = -0.6027968561678372\n","Epoch 11: w1 = -3.276785787876371, w2 = -0.5352538875577688\n","Epoch 12: w1 = -3.1760178230016334, w2 = -0.47326154058825287\n","Epoch 13: w1 = -3.078122901858806, w2 = -0.4164249923493902\n","Epoch 14: w1 = -2.9827682737992003, w2 = -0.3644266043432175\n","Epoch 15: w1 = -2.8896845602323835, w2 = -0.31700215000026094\n","Epoch 16: w1 = -2.7986515704878028, w2 = -0.2739231571205742\n","Epoch 17: w1 = -2.7094879893929695, w2 = -0.23498342145223505\n","Epoch 18: w1 = -2.622043725389032, w2 = -0.1999885400915886\n","Epoch 19: w1 = -2.5361941314420555, w2 = -0.16874779201996495\n","Epoch 20: w1 = -2.45183557249866, w2 = -0.14106798665812004\n","Epoch 21: w1 = -2.368881979526565, w2 = -0.11674907672147622\n","Epoch 22: w1 = -2.287262138705976, w2 = -0.09558142509934656\n","Epoch 23: w1 = -2.2069175368107667, w2 = -0.0773446484542851\n","Epoch 24: w1 = -2.1278006332192176, w2 = -0.0618079481242705\n","Epoch 25: w1 = -2.0498734632922044, w2 = -0.04873179546947124\n","Epoch 26: w1 = -1.9731065020644345, w2 = -0.03787077801213713\n","Epoch 27: w1 = -1.8974777345328453, w2 = -0.028977348753561286\n","Epoch 28: w1 = -1.822971891406658, w2 = -0.021806167132961803\n","Epoch 29: w1 = -1.7495798184140114, w2 = -0.016118686870236938\n","Epoch 30: w1 = -1.6772979540949833, w2 = -0.011687640137678012\n"]}]},{"cell_type":"markdown","source":["# Bài 4: Adam"],"metadata":{"id":"M2KMyuyhMiw0"}},{"cell_type":"code","source":["def df_w(W):\n","    \"\"\"\n","    Compute the gradents of dw1 and dw2.\n","    Arguments:\n","    W -- np.array [w1, w2]\n","    Returns:\n","    dw -- np.array [dw1, dw2], array containing the partial derivatives\n","    \"\"\"\n","    w1, w2 = W\n","\n","    dw1 = 2 * w1\n","    dw2 = 2 * w2\n","\n","    dw = np.array([dw1, dw2])\n","    return dw"],"metadata":{"id":"T60DlVqbMpOd","executionInfo":{"status":"ok","timestamp":1732636869638,"user_tz":-420,"elapsed":481,"user":{"displayName":"Hanh Mai","userId":"06375785904431938117"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["def adam(W, V, S, dw, lr, beta1, beta2, epsilon, t):\n","    \"\"\"\n","    Perform the Adam optimization update.\n","    Arguments:\n","    W -- np.array [w1, w2], weights\n","    V -- np.array [v1, v2], first moment estimate\n","    S -- np.array [s1, s2], second moment estimate\n","    dw -- np.array [dw1, dw2], gradients\n","    lr -- float, learning rate\n","    beta1 -- float, exponential decay rate for first moment estimates\n","    beta2 -- float, exponential decay rate for second moment estimates\n","    epsilon -- float, small constant to prevent division by zero\n","    t -- int, timestep\n","    Returns:\n","    W -- np.array [w1, w2] after update\n","    V -- np.array [v1, v2], updated first moment estimate\n","    S -- np.array [s1, s2], updated second moment estimate\n","    \"\"\"\n","    # Update biased first moment estimate (v_t)\n","    V = beta1 * V + (1 - beta1) * dw\n","    # Update biased second raw moment estimate (s_t)\n","    S = beta2 * S + (1 - beta2) * (dw ** 2)\n","    # Compute bias-corrected first moment estimate\n","    V_corr = V / (1 - beta1 ** t)\n","    # Compute bias-corrected second moment estimate\n","    S_corr = S / (1 - beta2 ** t)\n","    # Update weights\n","    W = W - (lr * V_corr) / (np.sqrt(S_corr) + epsilon)\n","    return W, V, S"],"metadata":{"id":"OITa9CRpMpwc","executionInfo":{"status":"ok","timestamp":1732636870555,"user_tz":-420,"elapsed":2,"user":{"displayName":"Hanh Mai","userId":"06375785904431938117"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["def train_p1_adam(lr, beta1, beta2, epsilon, epochs):\n","    \"\"\"\n","    Train to find the minimum using Adam optimization.\n","    Arguments:\n","    lr -- float, learning rate\n","    beta1 -- float, exponential decay rate for first moment estimates\n","    beta2 -- float, exponential decay rate for second moment estimates\n","    epsilon -- float, small constant to prevent division by zero\n","    epochs -- int, number of epochs\n","    Returns:\n","    results -- list of [w1, w2] values after each epoch\n","    \"\"\"\n","    # Initial point\n","    W = np.array([-5.0, -2.0], dtype=np.float32)  # Initial weights\n","    V = np.array([0.0, 0.0], dtype=np.float32)    # Initial first moment\n","    S = np.array([0.0, 0.0], dtype=np.float32)    # Initial second moment\n","    # List of results to store W at each epoch\n","    results = [W]\n","\n","    # Loop through epochs\n","    for epoch in range(1, epochs + 1):\n","        # Compute gradients\n","        dw = df_w(W)\n","        # Update weights using Adam optimization\n","        W, V, S = adam(W, V, S, dw, lr, beta1, beta2, epsilon, epoch)\n","        # Append the new weights to results\n","        results.append(W)\n","\n","    return results"],"metadata":{"id":"sl8ce3JWMpmc","executionInfo":{"status":"ok","timestamp":1732636872472,"user_tz":-420,"elapsed":508,"user":{"displayName":"Hanh Mai","userId":"06375785904431938117"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["# Parameters for Adam\n","learning_rate = 0.1  # Learning rate\n","beta1 = 0.9          # Exponential decay rate for first moment estimates\n","beta2 = 0.999        # Exponential decay rate for second moment estimates\n","epsilon = 1e-8       # Small constant to prevent division by zero\n","epochs = 30           # Number of epochs"],"metadata":{"id":"b0fZtbaNMpft","executionInfo":{"status":"ok","timestamp":1732636873674,"user_tz":-420,"elapsed":3,"user":{"displayName":"Hanh Mai","userId":"06375785904431938117"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","source":["# Perform Adam optimization for  epochs\n","results = train_p1_adam(learning_rate, beta1, beta2, epsilon, epochs)\n","\n","# Display the results\n","for i, W in enumerate(results):\n","    print(f\"Epoch {i}: w1 = {W[0]}, w2 = {W[1]}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hflDs8DlNFVN","executionInfo":{"status":"ok","timestamp":1732636875059,"user_tz":-420,"elapsed":4,"user":{"displayName":"Hanh Mai","userId":"06375785904431938117"}},"outputId":"5abc843c-2dd3-4dbd-9d52-7833f326af77"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 0: w1 = -5.0, w2 = -2.0\n","Epoch 1: w1 = -4.9000000001, w2 = -1.90000000025\n","Epoch 2: w1 = -4.800057756868857, w2 = -1.8001664861157012\n","Epoch 3: w1 = -4.700213291333392, w2 = -1.7006233920464653\n","Epoch 4: w1 = -4.600507673963063, w2 = -1.601504895289421\n","Epoch 5: w1 = -4.500982947818709, w2 = -1.5029557812950123\n","Epoch 6: w1 = -4.401682046815696, w2 = -1.4051317301601145\n","Epoch 7: w1 = -4.302648709562401, w2 = -1.3081994956139178\n","Epoch 8: w1 = -4.203927389258771, w2 = -1.2123369431278836\n","Epoch 9: w1 = -4.105563160155665, w2 = -1.1177329089643084\n","Epoch 10: w1 = -4.007601621083933, w2 = -1.0245868378253595\n","Epoch 11: w1 = -3.9100887965630893, w2 = -0.9331081538332233\n","Epoch 12: w1 = -3.8130710359932927, w2 = -0.8435153186154638\n","Epoch 13: w1 = -3.7165949114219345, w2 = -0.7560345321554123\n","Epoch 14: w1 = -3.620707114358172, w2 = -0.6708980377073323\n","Epoch 15: w1 = -3.5254543520861086, w2 = -0.5883420023105994\n","Epoch 16: w1 = -3.4308832439010026, w2 = -0.508603959872017\n","Epoch 17: w1 = -3.3370402176638723, w2 = -0.4319198245927328\n","Epoch 18: w1 = -3.243971407039156, w2 = -0.3585205082268796\n","Epoch 19: w1 = -3.1517225497487047, w2 = -0.28862820399255\n","Epoch 20: w1 = -3.060338887144194, w2 = -0.22245243074652704\n","Epoch 21: w1 = -2.969865065369962, w2 = -0.1601859603146566\n","Epoch 22: w1 = -2.8803450383600078, w2 = -0.10200077514951103\n","Epoch 23: w1 = -2.7918219728870515, w2 = -0.04804421922755551\n","Epoch 24: w1 = -2.7043381558586734, w2 = 0.0015644906787498358\n","Epoch 25: w1 = -2.6179349040359683, w2 = 0.04673723530732809\n","Epoch 26: w1 = -2.5326524763340537, w2 = 0.08741930927401395\n","Epoch 27: w1 = -2.448529988851293, w2 = 0.12359098221781599\n","Epoch 28: w1 = -2.365605332765134, w2 = 0.15526824595178323\n","Epoch 29: w1 = -2.2839150952268548, w2 = 0.18250273504689696\n","Epoch 30: w1 = -2.2034944833849917, w2 = 0.20538085318546184\n"]}]}]}