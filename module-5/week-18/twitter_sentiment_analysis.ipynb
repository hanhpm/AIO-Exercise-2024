{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zU6JOHKSbdsc",
        "outputId": "a9271473-3764-4f4c-eb85-14983b2ed068"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = 'Twitter_Data.csv'\n",
        "df = pd.read_csv(dataset_path)\n",
        "df.head()\n",
        "df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vHzc_S0Cb9ZQ",
        "outputId": "9f679d69-fde4-4d5a-f6aa-43806c609efa"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 162980 entries, 0 to 162979\n",
            "Data columns (total 2 columns):\n",
            " #   Column      Non-Null Count   Dtype  \n",
            "---  ------      --------------   -----  \n",
            " 0   clean_text  162976 non-null  object \n",
            " 1   category    162973 non-null  float64\n",
            "dtypes: float64(1), object(1)\n",
            "memory usage: 2.5+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.dropna()"
      ],
      "metadata": {
        "id": "X79klYvhb9cL"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def text_normalize(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'^rt[\\s]+', '', text)\n",
        "    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text)\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = text.split()\n",
        "    words = [word for word in words if word not in stop_words]\n",
        "    text = ' '.join(words)\n",
        "    stemmer = SnowballStemmer('english')\n",
        "    words = text.split()\n",
        "    words = [stemmer.stem(word) for word in words]\n",
        "    text = ' '.join(words)\n",
        "    return text\n",
        "\n",
        "vectorizer = TfidfVectorizer(max_features=2000)\n",
        "X = vectorizer.fit_transform(df['clean_text']).toarray()\n",
        "\n",
        "intercept = np.ones((X.shape[0],1))\n",
        "X_b = np.concatenate((intercept, X), axis=1)"
      ],
      "metadata": {
        "id": "b1RDRUgeb9el"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_classes = df['category'].nunique()\n",
        "n_samples = df['category'].size\n",
        "\n",
        "y = df['category'].to_numpy() + 1\n",
        "y = y.astype(np.uint8)\n",
        "y_encoded = np.array([np.zeros(n_classes) for _ in range(n_samples)])\n",
        "y_encoded[np.arange(n_samples), y] = 1\n",
        "\n",
        "val_size = 0.2\n",
        "test_size = 0.125\n",
        "random_state = 2\n",
        "is_shuffle = True\n",
        "\n",
        "# Split data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_b, y_encoded,\n",
        "    test_size=val_size,\n",
        "    random_state=random_state,\n",
        "    shuffle=is_shuffle\n",
        ")\n",
        "\n",
        "# Further split training set into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_train, y_train,\n",
        "    test_size=test_size,\n",
        "    random_state=random_state,\n",
        "    shuffle=is_shuffle\n",
        ")"
      ],
      "metadata": {
        "id": "-IOCwxuIb9hO"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(z):\n",
        "    exp_z = np.exp(z)\n",
        "    return exp_z / exp_z.sum(axis=1)[:, None]\n",
        "\n",
        "def compute_loss(y_hat, y):\n",
        "    n = y.size\n",
        "    return (-1 / n) * np.sum(y * np.log(y_hat))\n",
        "\n",
        "def predict(X, theta):\n",
        "    z = np.dot(X, theta)\n",
        "    y_hat = softmax(z)\n",
        "    return y_hat\n",
        "\n",
        "def compute_gradient(X, y, y_hat):\n",
        "    n = y.size\n",
        "    return np.dot(X.T, (y_hat - y)) / n\n",
        "\n",
        "def update_theta(theta, gradient, lr):\n",
        "    return theta - lr * gradient\n",
        "\n",
        "def compute_accuracy(X, y, theta):\n",
        "    y_hat = predict(X, theta)\n",
        "    acc = (np.argmax(y_hat, axis=1) == np.argmax(y, axis=1)).mean()\n",
        "    return acc"
      ],
      "metadata": {
        "id": "DmQBLMH1b9j2"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 0.1\n",
        "epochs = 200\n",
        "batch_size = X_train.shape[0]\n",
        "n_features = X_train.shape[1]\n",
        "\n",
        "np.random.seed(random_state)\n",
        "theta = np.random.uniform(size=(n_features, n_classes))"
      ],
      "metadata": {
        "id": "czWf0K-Db9mX"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_set_acc = compute_accuracy(X_val, y_val, theta)\n",
        "test_set_acc = compute_accuracy(X_test, y_test, theta)\n",
        "print('Evaluation on validation and test set:')\n",
        "print(f'Validation Accuracy: {val_set_acc}')\n",
        "print(f'Test Accuracy: {test_set_acc}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EuXaszniea0Y",
        "outputId": "9339c6f7-fbe2-4e6c-d63a-9efef0df9954"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation on validation and test set:\n",
            "Validation Accuracy: 0.36577284162729334\n",
            "Test Accuracy: 0.3641160949868074\n"
          ]
        }
      ]
    }
  ]
}