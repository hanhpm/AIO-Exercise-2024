{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"pGoINa7J1pZt"},"outputs":[],"source":["import random\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import torch\n","import torch.optim as optim\n","import torchvision\n","import torchvision.transforms as transforms\n","from torch import nn\n","from torch.utils.data import DataLoader, Dataset, random_split\n","from torchvision.datasets import FashionMNIST"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jvU8mHvW2Fa5"},"outputs":[],"source":["device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","\n","def set_seed(seed):\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","\n","SEED = 42\n","set_seed(SEED)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9791,"status":"ok","timestamp":1733500069442,"user":{"displayName":"Hanh Mai","userId":"06375785904431938117"},"user_tz":-420},"id":"sxY3asSK4GBV","outputId":"92d5dcf4-c995-49ac-b2c1-d44bade261c2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n","Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 26.4M/26.4M [00:03<00:00, 7.88MB/s]\n"]},{"name":"stdout","output_type":"stream","text":["Extracting ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n","\n","Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n","Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 29.5k/29.5k [00:00<00:00, 138kB/s]\n"]},{"name":"stdout","output_type":"stream","text":["Extracting ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n","\n","Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n","Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 4.42M/4.42M [00:01<00:00, 2.47MB/s]\n"]},{"name":"stdout","output_type":"stream","text":["Extracting ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n","\n","Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n","Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 5.15k/5.15k [00:00<00:00, 4.68MB/s]"]},{"name":"stdout","output_type":"stream","text":["Extracting ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n","\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["train_dataset = FashionMNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n","test_dataset = FashionMNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1733500069442,"user":{"displayName":"Hanh Mai","userId":"06375785904431938117"},"user_tz":-420},"id":"WNeuYvXX4GE2","outputId":"1363958a-c85f-4379-f959-0cc876981251"},"outputs":[{"name":"stdout","output_type":"stream","text":["Train size: 54000\n","Validation size: 6000\n","Test size: 10000\n"]}],"source":["train_ratio = 0.9\n","train_size = int(train_ratio * len(train_dataset))\n","val_size = len(train_dataset) - train_size\n","\n","train_subset, val_subset =  random_split(train_dataset, [train_size, val_size])\n","\n","train_loader = DataLoader(train_subset, batch_size=32, shuffle=True)\n","val_loader = DataLoader(val_subset, batch_size=32, shuffle=False)\n","test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n","\n","print(f\"Train size: {len(train_subset)}\")\n","print(f\"Validation size: {len(val_subset)}\")\n","print(f\"Test size: {len(test_dataset)}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DwLM4JNQ4GI7"},"outputs":[],"source":["# class MLP(nn.Module):\n","#     def __init__(self, input_dims, hidden_dims, output_dims):\n","#         super(MLP, self).__init__()\n","#         self.layer1 = nn.Linear(input_dims, hidden_dims)\n","#         self.layer2 = nn.Linear(hidden_dims, hidden_dims)\n","#         self.layer3 = nn.Linear(hidden_dims, hidden_dims)\n","#         self.layer4 = nn.Linear(hidden_dims, hidden_dims)\n","#         self.layer5 = nn.Linear(hidden_dims, hidden_dims)\n","#         self.layer6 = nn.Linear(hidden_dims, hidden_dims)\n","#         self.layer7 = nn.Linear(hidden_dims, hidden_dims)\n","#         self.output = nn.Linear(hidden_dims, output_dims)\n","\n","#         # # Iterate over all modules in the model\n","#         # for module in self.modules():\n","#         #     if isinstance(module, nn.Linear):\n","#         #         # Initialize weights with a normal distribution (mean=0.0, std=1.0)\n","#         #         nn.init.normal_(module.weight, mean=0.0, std=1.0)\n","\n","#         #         # Initialize biases to a constant value of 0.0\n","#         #         nn.init.constant_(module.bias, 0.0)\n","\n","#         # for module in self.modules():\n","#         #     if isinstance(module, nn.Linear):\n","#         #         nn.init.normal_(module.weight, mean=0.0, std=10.0)\n","#         #         nn.init.constant_(module.bias, 0.0)\n","\n","#         for module in self.modules():\n","#             if isinstance(module, nn.Linear):\n","#                 nn.init.normal_(module.weight, mean=0.0, std=05.0)\n","#                 nn.init.constant_(module.bias, 0.0)\n","\n","\n","#     def forward(self, x):\n","#         x = nn.Flatten()(x)\n","#         x = self.layer1(x)\n","#         x = nn.Sigmoid()(x)\n","#         x = self.layer2(x)\n","#         x = nn.Sigmoid()(x)\n","#         x = self.layer3(x)\n","#         x = nn.Sigmoid()(x)\n","#         x = self.layer4(x)\n","#         x = nn.Sigmoid()(x)\n","#         x = self.layer5(x)\n","#         x = nn.Sigmoid()(x)\n","#         x = self.layer6(x)\n","#         x = nn.Sigmoid()(x)\n","#         x = self.layer7(x)\n","#         x = nn.Sigmoid()(x)\n","#         out = self.output(x)\n","#         return out\n","\n","# class MLP(nn.Module):\n","#     def __init__(self, input_dims, hidden_dims, output_dims):\n","#         super(MLP, self).__init__()\n","#         self.hidden_dims = hidden_dims\n","\n","#         # Define layers\n","#         self.layer1 = nn.Linear(input_dims, hidden_dims)\n","#         self.bn1 = nn.BatchNorm1d(hidden_dims)\n","#         self.layer2 = nn.Linear(hidden_dims, hidden_dims)\n","#         self.bn2 = nn.BatchNorm1d(hidden_dims)\n","#         self.layer3 = nn.Linear(hidden_dims, hidden_dims)\n","#         self.bn3 = nn.BatchNorm1d(hidden_dims)\n","#         self.layer4 = nn.Linear(hidden_dims, hidden_dims)\n","#         self.bn4 = nn.BatchNorm1d(hidden_dims)\n","#         self.layer5 = nn.Linear(hidden_dims, hidden_dims)\n","#         self.bn5 = nn.BatchNorm1d(hidden_dims)\n","#         self.layer6 = nn.Linear(hidden_dims, hidden_dims)\n","#         self.bn6 = nn.BatchNorm1d(hidden_dims)\n","#         self.layer7 = nn.Linear(hidden_dims, hidden_dims)\n","#         self.bn7 = nn.BatchNorm1d(hidden_dims)\n","#         self.output = nn.Linear(hidden_dims, output_dims)\n","\n","#         # Initialize weights and biases\n","#         for module in self.modules():\n","#             if isinstance(module, nn.Linear):\n","#                 nn.init.normal_(module.weight, mean=0.0, std=0.05)\n","#                 nn.init.constant_(module.bias, 0.0)\n","\n","#     def forward(self, x):\n","#         # Flatten the input\n","#         x = nn.Flatten()(x)\n","\n","#         # Forward pass through each layer with BatchNorm and activation\n","#         x = self.bn1(self.layer1(x))\n","#         x = nn.Sigmoid()(x)\n","\n","#         x = self.bn2(self.layer2(x))\n","#         x = nn.Sigmoid()(x)\n","\n","#         x = self.bn3(self.layer3(x))\n","#         x = nn.Sigmoid()(x)\n","\n","#         x = self.bn4(self.layer4(x))\n","#         x = nn.Sigmoid()(x)\n","\n","#         x = self.bn5(self.layer5(x))\n","#         x = nn.Sigmoid()(x)\n","\n","#         x = self.bn6(self.layer6(x))\n","#         x = nn.Sigmoid()(x)\n","\n","#         x = self.bn7(self.layer7(x))\n","#         x = nn.Sigmoid()(x)\n","\n","#         # Output layer\n","#         out = self.output(x)\n","#         return out\n","\n","# RuntimeError: Expected all tensors to be on the same device, but found at\n","# least two devices, cuda:0 and cpu! (when checking argument for argument\n","# weight in method wrapper_CUDA__native_batch_norm)\n","\n","# class MyNormalization(nn.Module):\n","#     def __init__(self):\n","#         super(MyNormalization, self).__init__()\n","\n","#     def forward(self, x):\n","#         # Calculate mean and standard deviation of the input\n","#         mean = torch.mean(x, dim=0, keepdim=True)  # Calculate along batch dimension\n","#         std = torch.std(x, dim=0, keepdim=True)\n","#         # Normalize the input\n","#         return (x - mean) / (std + 1e-5)  # Add epsilon for numerical stability\n","\n","# class MLP(nn.Module):\n","#     def __init__(self, input_dims, hidden_dims, output_dims):\n","#         super(MLP, self).__init__()\n","#         self.hidden_dims = hidden_dims\n","\n","#         # Define layers and normalization layers\n","#         self.layer1 = nn.Linear(input_dims, hidden_dims)\n","#         self.norm1 = MyNormalization()\n","\n","#         self.layer2 = nn.Linear(hidden_dims, hidden_dims)\n","#         self.norm2 = MyNormalization()\n","\n","        # self.layer3 = nn.Linear(hidden_dims, hidden_dims)\n","        # self.norm3 = MyNormalization()\n","\n","        # self.layer4 = nn.Linear(hidden_dims, hidden_dims)\n","        # self.norm4 = MyNormalization()\n","\n","        # self.layer5 = nn.Linear(hidden_dims, hidden_dims)\n","        # self.norm5 = MyNormalization()\n","\n","        # self.layer6 = nn.Linear(hidden_dims, hidden_dims)\n","        # self.norm6 = MyNormalization()\n","\n","        # self.layer7 = nn.Linear(hidden_dims, hidden_dims)\n","        # self.norm7 = MyNormalization()\n","\n","        # self.output = nn.Linear(hidden_dims, output_dims)\n","\n","        # Initialize weights and biases\n","        # for module in self.modules():\n","        #     if isinstance(module, nn.Linear):\n","        #         nn.init.normal_(module.weight, mean=0.0, std=0.05)\n","        #         nn.init.constant_(module.bias, 0.0)\n","\n","    # def forward(self, x):\n","    #     # Flatten the input\n","        # x = nn.Flatten()(x)\n","\n","        # # Forward pass through layers with custom normalization and activation\n","        # x = self.norm1(self.layer1(x))\n","        # x = nn.Sigmoid()(x)\n","\n","        # x = self.norm2(self.layer2(x))\n","        # x = nn.Sigmoid()(x)\n","\n","        # x = self.norm3(self.layer3(x))\n","        # x = nn.Sigmoid()(x)\n","\n","        # x = self.norm4(self.layer4(x))\n","        # x = nn.Sigmoid()(x)\n","\n","        # x = self.norm5(self.layer5(x))\n","        # x = nn.Sigmoid()(x)\n","\n","        # x = self.norm6(self.layer6(x))\n","        # x = nn.Sigmoid()(x)\n","\n","        # x = self.norm7(self.layer7(x))\n","        # x = nn.Sigmoid()(x)\n","\n","        # # Output layer\n","        # out = self.output(x)\n","        # return out\n","\n","class MLP(nn.Module):\n","    def __init__(self, input_dims, hidden_dims, output_dims):\n","        super(MLP, self).__init__()\n","\n","        # Define layers\n","        self.layer1 = nn.Linear(input_dims, hidden_dims)\n","        self.layer2 = nn.Linear(hidden_dims, hidden_dims)\n","        self.layer3 = nn.Linear(hidden_dims, hidden_dims)\n","        self.layer4 = nn.Linear(hidden_dims, hidden_dims)\n","        self.layer5 = nn.Linear(hidden_dims, hidden_dims)\n","        self.layer6 = nn.Linear(hidden_dims, hidden_dims)\n","        self.layer7 = nn.Linear(hidden_dims, hidden_dims)\n","        self.output = nn.Linear(hidden_dims, output_dims)\n","\n","        # Initialize weights and biases\n","        for module in self.modules():\n","            if isinstance(module, nn.Linear):\n","                nn.init.normal_(module.weight, mean=0.0, std=0.05)\n","                nn.init.constant_(module.bias, 0.0)\n","\n","    def forward(self, x):\n","        # Flatten the input\n","        x = nn.Flatten()(x)\n","\n","        # Layer 1 with activation\n","        x = self.layer1(x)\n","        x = nn.Sigmoid()(x)\n","        skip = x  # Save for residual connection\n","\n","        # Layers 2 and 3 with residual connection\n","        x = self.layer2(x)\n","        x = nn.Sigmoid()(x)\n","        x = self.layer3(x)\n","        x = nn.Sigmoid()(x)\n","        x = skip + x  # Add skip connection\n","\n","        # Layer 4 with activation\n","        x = self.layer4(x)\n","        x = nn.Sigmoid()(x)\n","        skip = x  # Save for residual connection\n","\n","        # Layers 5, 6, and 7 with residual connection\n","        x = self.layer5(x)\n","        x = nn.Sigmoid()(x)\n","        x = self.layer6(x)\n","        x = nn.Sigmoid()(x)\n","        x = self.layer7(x)\n","        x = nn.Sigmoid()(x)\n","        x = skip + x  # Add skip connection\n","\n","        # Output layer\n","        out = self.output(x)\n","        return out\n","\n","# Parameters\n","input_dims = 784\n","hidden_dims = 128\n","output_dims = 10\n","lr = 1e-3\n","\n","# Model Initialization\n","model = MLP(\n","    input_dims=input_dims,\n","    hidden_dims=hidden_dims,\n","    output_dims=output_dims\n",").to(device)\n","\n","# Loss and Optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=lr)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"--66aGJO4GM8","outputId":"36361804-6e4f-42d8-e27e-e13320e223dc"},"outputs":[{"name":"stdout","output_type":"stream","text":["EPOCH 1/100, Train Loss: 0.6637, Train Acc: 0.8021, Validation Loss: 0.5315, Val Acc: 0.8197\n","EPOCH 2/100, Train Loss: 0.4744, Train Acc: 0.8351, Validation Loss: 0.4790, Val Acc: 0.8335\n","EPOCH 3/100, Train Loss: 0.4471, Train Acc: 0.8431, Validation Loss: 0.4481, Val Acc: 0.8422\n","EPOCH 4/100, Train Loss: 0.4264, Train Acc: 0.8476, Validation Loss: 0.4410, Val Acc: 0.8427\n","EPOCH 5/100, Train Loss: 0.4082, Train Acc: 0.8544, Validation Loss: 0.4290, Val Acc: 0.8468\n","EPOCH 6/100, Train Loss: 0.3929, Train Acc: 0.8599, Validation Loss: 0.4170, Val Acc: 0.8557\n","EPOCH 7/100, Train Loss: 0.3864, Train Acc: 0.8618, Validation Loss: 0.4145, Val Acc: 0.8560\n"]}],"source":["epochs = 100\n","\n","# Metrics storage\n","train_loss_lst = []\n","train_acc_lst = []\n","val_loss_lst = []\n","val_acc_lst = []\n","\n","# Training and Validation Loop\n","for epoch in range(epochs):\n","    train_loss = 0.0\n","    train_acc = 0.0\n","    count = 0\n","\n","    # Training Phase\n","    model.train()\n","    for X_train, y_train in train_loader:\n","        # Move data to device\n","        X_train, y_train = X_train.to(device), y_train.to(device)\n","\n","        # Zero gradients\n","        optimizer.zero_grad()\n","\n","        # Forward pass\n","        outputs = model(X_train)\n","        loss = criterion(outputs, y_train)\n","\n","        # Backward pass and optimization\n","        loss.backward()\n","        optimizer.step()\n","\n","        # Track training loss and accuracy\n","        train_loss += loss.item()\n","        train_acc += (torch.argmax(outputs, 1) == y_train).sum().item()\n","        count += len(y_train)\n","\n","    # Average training metrics\n","    train_loss /= len(train_loader)\n","    train_loss_lst.append(train_loss)\n","    train_acc /= count\n","    train_acc_lst.append(train_acc)\n","\n","    # Validation Phase\n","    val_loss = 0.0\n","    val_acc = 0.0\n","    count = 0\n","    model.eval()\n","    with torch.no_grad():\n","        for X_val, y_val in val_loader:\n","            # Move data to device\n","            X_val, y_val = X_val.to(device), y_val.to(device)\n","\n","            # Forward pass\n","            outputs = model(X_val)\n","            loss = criterion(outputs, y_val)\n","\n","            # Track validation loss and accuracy\n","            val_loss += loss.item()\n","            val_acc += (torch.argmax(outputs, 1) == y_val).sum().item()\n","            count += len(y_val)\n","\n","    # Average validation metrics\n","    val_loss /= len(val_loader)\n","    val_loss_lst.append(val_loss)\n","    val_acc /= count\n","    val_acc_lst.append(val_acc)\n","\n","    # Log epoch metrics\n","    print(f\"EPOCH {epoch+1}/{epochs}, \"\n","          f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, \"\n","          f\"Validation Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AbkhG4tv4GRS"},"outputs":[],"source":["fig, ax = plt.subplots(2, 2, figsize=(12, 10))\n","\n","# Training Loss\n","ax[0, 0].plot(train_loss_lst, color='green')\n","ax[0, 0].set(xlabel='Epoch', ylabel='Loss')\n","ax[0, 0].set_title('Training Loss')\n","\n","# Validation Loss\n","ax[0, 1].plot(val_loss_lst, color='orange')\n","ax[0, 1].set(xlabel='Epoch', ylabel='Loss')\n","ax[0, 1].set_title('Validation Loss')\n","\n","# Training Accuracy\n","ax[1, 0].plot(train_acc_lst, color='green')\n","ax[1, 0].set(xlabel='Epoch', ylabel='Accuracy')\n","ax[1, 0].set_title('Training Accuracy')\n","\n","# Validation Accuracy\n","ax[1, 1].plot(val_acc_lst, color='orange')\n","ax[1, 1].set(xlabel='Epoch', ylabel='Accuracy')\n","ax[1, 1].set_title('Validation Accuracy')\n","\n","# Display the plot\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uAE9E14P6NMf"},"outputs":[],"source":["# Initialize lists for storing predictions and targets\n","test_target = []\n","test_predict = []\n","\n","# Set model to evaluation mode\n","model.eval()\n","\n","# Disable gradient computation for testing\n","with torch.no_grad():\n","    for X_test, y_test in test_loader:\n","        # Move data to the appropriate device (GPU/CPU)\n","        X_test = X_test.to(device)\n","        y_test = y_test.to(device)\n","\n","        # Forward pass\n","        outputs = model(X_test)\n","\n","        # Collect predictions and targets\n","        test_predict.append(outputs.cpu())\n","        test_target.append(y_test.cpu())\n","\n","# Concatenate all predictions and targets\n","test_predict = torch.cat(test_predict)\n","test_target = torch.cat(test_target)\n","\n","# Compute accuracy\n","test_acc = (torch.argmax(test_predict, 1) == test_target).sum().item() / len(test_target)\n","\n","# Print evaluation results\n","print('Evaluation on test set:')\n","print(f'Accuracy: {test_acc:.4f}')"]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMJtWb3qMJ7rzNsPIHXmg8X"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}