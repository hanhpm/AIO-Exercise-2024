{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOoP1ZkwItoSm3/VHM8ZaiD"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":211},"id":"CJ52T0yey90b","executionInfo":{"status":"error","timestamp":1743351971497,"user_tz":-420,"elapsed":113,"user":{"displayName":"Hanh Mai","userId":"06375785904431938117"}},"outputId":"689fd0c2-2faf-40fc-a672-ebb6507f0ab6"},"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: './img_align_celeba'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-d795c9696e35>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# List all image paths in the directory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mfile_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./img_align_celeba'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mimg_paths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'./img_align_celeba/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfile_name\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfile_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfile_names\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './img_align_celeba'"]}],"source":["# Import required libraries\n","import os\n","import numpy as np\n","\n","# List all image paths in the directory\n","file_names = os.listdir('./img_align_celeba')\n","img_paths = ['./img_align_celeba/' + file_name for file_name in file_names]\n","\n","# Split dataset into training and validation sets\n","num_train = 150000\n","train_imgpaths = img_paths[:num_train]\n","val_imgpaths = img_paths[num_train:]\n","\n","# Function to generate mask image from bounding box\n","def bbox2mask(img_shape, bbox, dtype='uint8'):\n","    \"\"\"\n","    Generate mask in ndarray from bbox.\n","\n","    Args:\n","        img_shape (tuple[int]): Shape of the image (height, width, channels).\n","        bbox (tuple[int]): Configuration tuple, (top, left, height, width).\n","        dtype (str): Data type of the mask array.\n","\n","    Returns:\n","        np.ndarray: Mask array.\n","    \"\"\"\n","    height, width = img_shape[:2]\n","    mask = np.zeros((height, width, 1), dtype=dtype)\n","    mask[bbox[0]:bbox[0] + bbox[2], bbox[1]:bbox[1] + bbox[3], :] = 1\n","    return mask"]},{"cell_type":"code","source":["import torch\n","from PIL import Image\n","from torchvision import transforms\n","from torch.utils.data import Dataset\n","\n","# Build dataset\n","class InpaintingDataset(Dataset):\n","    def __init__(self, img_paths, mask_mode, image_size=[256, 256]):\n","        \"\"\"\n","        Args:\n","            img_paths (list): List of image file paths.\n","            mask_mode (str): Mode for generating masks (e.g., 'center').\n","            image_size (list): Dimensions to resize the image to [height, width].\n","        \"\"\"\n","        self.img_paths = img_paths\n","        self.tfs = transforms.Compose([\n","            transforms.Resize((image_size[0], image_size[1])),\n","            transforms.ToTensor(),\n","            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n","        ])\n","        self.mask_mode = mask_mode\n","        self.image_size = image_size\n","\n","    def __getitem__(self, index):\n","        \"\"\"\n","        Fetches an image and its corresponding mask.\n","\n","        Args:\n","            index (int): Index of the image to fetch.\n","\n","        Returns:\n","            dict: Dictionary containing:\n","                  - 'gt_image': Original ground truth image.\n","                  - 'cond_image': Image with random noise applied to the masked area.\n","                  - 'mask_image': Ground truth image with mask applied.\n","                  - 'mask': Mask generated for the image.\n","                  - 'path': File path of the image.\n","        \"\"\"\n","        img_path = self.img_paths[index]\n","        img = Image.open(img_path).convert('RGB')\n","        img = self.tfs(img)\n","\n","        mask = self.get_mask()\n","        cond_image = img * (1. - mask) + mask * torch.randn_like(img)\n","        mask_img = img * (1. - mask) + mask\n","\n","        return {\n","            'gt_image': img,\n","            'cond_image': cond_image,\n","            'mask_image': mask_img,\n","            'mask': mask,\n","            'path': img_path\n","        }\n","\n","    def __len__(self):\n","        \"\"\"\n","        Returns:\n","            int: Total number of images in the dataset.\n","        \"\"\"\n","        return len(self.img_paths)\n","\n","    def get_mask(self):\n","        \"\"\"\n","        Generates a mask based on the mask mode.\n","\n","        Returns:\n","            torch.Tensor: Mask tensor.\n","        \"\"\"\n","        if self.mask_mode == 'center':\n","            h, w = self.image_size\n","            mask = bbox2mask(self.image_size, (h // 4, w // 4, h // 2, w // 2))\n","        else:\n","            raise NotImplementedError(\n","                f'Mask mode {self.mask_mode} has not been implemented.'\n","            )\n","        return torch.from_numpy(mask).permute(2, 0, 1)"],"metadata":{"id":"xsFP_xJnzKfu","executionInfo":{"status":"aborted","timestamp":1743351971544,"user_tz":-420,"elapsed":336,"user":{"displayName":"Hanh Mai","userId":"06375785904431938117"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torch import nn\n","import math\n","\n","# Block class definition\n","class Block(nn.Module):\n","    def __init__(self, in_ch, out_ch, time_emb_dim, up=False):\n","        super().__init__()\n","        self.time_mlp = nn.Linear(time_emb_dim, out_ch)\n","        if up:\n","            self.conv1 = nn.Conv2d(2 * in_ch, out_ch, 3, padding=1)\n","            self.transform = nn.ConvTranspose2d(out_ch, out_ch, 4, 2, 1)\n","        else:\n","            self.conv1 = nn.Conv2d(in_ch, out_ch, 3, padding=1)\n","            self.transform = nn.Conv2d(out_ch, out_ch, 4, 2, 1)\n","        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1)\n","        self.bnorm1 = nn.BatchNorm2d(out_ch)\n","        self.bnorm2 = nn.BatchNorm2d(out_ch)\n","        self.relu = nn.ReLU()\n","\n","    def forward(self, x, t):\n","        h = self.bnorm1(self.relu(self.conv1(x)))\n","        time_emb = self.relu(self.time_mlp(t))  # Time embedding\n","        time_emb = time_emb[(...,) + (None,) * 2]\n","        h = h + time_emb  # Add time channel\n","        h = self.bnorm2(self.relu(self.conv2(h)))\n","        return self.transform(h)  # Down or Upsample\n","\n","\n","# SinusoidalPositionEmbeddings class definition\n","class SinusoidalPositionEmbeddings(nn.Module):\n","    def __init__(self, dim):\n","        super().__init__()\n","        self.dim = dim\n","\n","    def forward(self, time):\n","        device = time.device\n","        half_dim = self.dim // 2\n","        embeddings = math.log(10000) / (half_dim - 1)\n","        embeddings = torch.exp(torch.arange(half_dim, device=device) * embeddings)\n","        embeddings = time[:, None] * embeddings[None, :]\n","        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)\n","        return embeddings\n","\n","\n","# SimpleUnet class definition\n","class SimpleUnet(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        image_channels = 3\n","        down_channels = (64, 128, 256, 512, 1024)  # Channels for the downsampling path\n","        up_channels = (1024, 512, 256, 128, 64)    # Channels for the upsampling path\n","        out_dim = 3\n","        time_emb_dim = 32\n","\n","        # Time embedding\n","        self.time_mlp = nn.Sequential(\n","            SinusoidalPositionEmbeddings(time_emb_dim),\n","            nn.Linear(time_emb_dim, time_emb_dim),\n","            nn.ReLU()\n","        )\n","\n","        # Initial convolution\n","        self.conv0 = nn.Conv2d(image_channels, down_channels[0], 3, padding=1)\n","\n","        # Downsampling blocks\n","        self.downs = nn.ModuleList([\n","            Block(down_channels[i], down_channels[i + 1], time_emb_dim)\n","            for i in range(len(down_channels) - 1)\n","        ])\n","\n","        # Upsampling blocks\n","        self.ups = nn.ModuleList([\n","            Block(up_channels[i], up_channels[i + 1], time_emb_dim, up=True)\n","            for i in range(len(up_channels) - 1)\n","        ])\n","\n","        # Output layer\n","        self.output = nn.Conv2d(up_channels[-1], out_dim, 1)\n","\n","    def forward(self, x, timestep):\n","        # Initial convolution\n","        x = self.conv0(x)\n","\n","        # Time embedding\n","        t = self.time_mlp(timestep)\n","\n","        # Downsampling with residual connections\n","        residual_inputs = []\n","        for down in self.downs:\n","            x = down(x, t)\n","            residual_inputs.append(x)\n","\n","        # Upsampling with residual connections\n","        for up in self.ups:\n","            residual_x = residual_inputs.pop()\n","            x = torch.cat((x, residual_x), dim=1)\n","            x = up(x, t)\n","\n","        return self.output(x)"],"metadata":{"id":"2dVQBX0AzKce","executionInfo":{"status":"aborted","timestamp":1743351971547,"user_tz":-420,"elapsed":338,"user":{"displayName":"Hanh Mai","userId":"06375785904431938117"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from tqdm import tqdm\n","from functools import partial\n","import numpy as np\n","import torch\n","\n","def make_beta_schedule(schedule, n_timestep, linear_start=1e-5, linear_end=1e-2):\n","    \"\"\"\n","    Generate a beta schedule for the diffusion process.\n","\n","    Args:\n","        schedule (str): Type of schedule ('linear').\n","        n_timestep (int): Number of timesteps in the schedule.\n","        linear_start (float): Starting value for the linear schedule.\n","        linear_end (float): Ending value for the linear schedule.\n","\n","    Returns:\n","        np.ndarray: Array of betas for each timestep.\n","    \"\"\"\n","    if schedule == 'linear':\n","        betas = np.linspace(\n","            linear_start, linear_end, n_timestep, dtype=np.float64\n","        )\n","    else:\n","        raise NotImplementedError(f\"Schedule type '{schedule}' is not implemented.\")\n","    return betas\n","\n","def get_index_from_list(vals, t, x_shape=(1, 1, 1, 1)):\n","    \"\"\"\n","    Returns a specific index `t` of a list of values `vals`,\n","    while considering the batch dimension.\n","\n","    Args:\n","        vals (torch.Tensor): List of values.\n","        t (torch.Tensor): Indices to gather.\n","        x_shape (tuple): Shape for the output.\n","\n","    Returns:\n","        torch.Tensor: Values gathered from `vals` at indices `t`.\n","    \"\"\"\n","    batch_size, *_ = t.shape\n","    out = vals.gather(-1, t)\n","    return out.reshape(batch_size, *((1,) * (len(x_shape) - 1))).to(device)"],"metadata":{"id":"WLAPtoykzKZs","executionInfo":{"status":"aborted","timestamp":1743351971549,"user_tz":-420,"elapsed":340,"user":{"displayName":"Hanh Mai","userId":"06375785904431938117"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","from torch import nn\n","import numpy as np\n","from functools import partial\n","\n","class InpaintingGaussianDiffusion(nn.Module):\n","    def __init__(self, unet_config, beta_schedule, **kwargs):\n","        super(InpaintingGaussianDiffusion, self).__init__(**kwargs)\n","        self.denoise_fn = UNet(**unet_config)  # Denoising UNet model\n","        self.beta_schedule = beta_schedule\n","\n","    def set_new_noise_schedule(self, device):\n","        # Convert to torch tensors\n","        to_torch = partial(torch.tensor, dtype=torch.float32, device=device)\n","        betas = make_beta_schedule(**self.beta_schedule)\n","        alphas = 1. - betas\n","\n","        # Set timesteps and calculate cumulative products\n","        timesteps, = betas.shape\n","        self.num_timesteps = int(timesteps)\n","        gammas = np.cumprod(alphas, axis=0)\n","        gammas_prev = np.append(1., gammas[:-1])\n","\n","        # Register buffers for diffusion process parameters\n","        self.register_buffer(\"gammas\", to_torch(gammas))\n","        self.register_buffer(\"sqrt_recip_gammas\", to_torch(np.sqrt(1. / gammas)))\n","        self.register_buffer(\"sqrt_recipm1_gammas\", to_torch(np.sqrt(1. / gammas - 1)))\n","        posterior_variance = betas * (1. - gammas_prev) / (1. - gammas)\n","        self.register_buffer(\"posterior_log_variance_clipped\", to_torch(np.log(np.maximum(posterior_variance, 1e-20))))\n","        self.register_buffer(\"posterior_mean_coef1\", to_torch(betas * np.sqrt(gammas_prev) / (1. - gammas)))\n","        self.register_buffer(\"posterior_mean_coef2\", to_torch((1. - gammas_prev) * np.sqrt(alphas) / (1. - gammas)))\n","\n","    def set_loss(self, loss_fn):\n","        # Set the loss function\n","        self.loss_fn = loss_fn\n","\n","    def predict_start_from_noise(self, y_t, t, noise):\n","        # Predict start using noise\n","        return (\n","            get_index_from_list(self.sqrt_recip_gammas, t, y_t.shape) * y_t -\n","            get_index_from_list(self.sqrt_recipm1_gammas, t, y_t.shape) * noise\n","        )\n","\n","    def q_posterior(self, y_0_hat, y_t, t):\n","        # Compute posterior distribution\n","        posterior_mean = (\n","            get_index_from_list(self.posterior_mean_coef1, t, y_t.shape) * y_0_hat +\n","            get_index_from_list(self.posterior_mean_coef2, t, y_t.shape) * y_t\n","        )\n","        posterior_log_variance_clipped = get_index_from_list(self.posterior_log_variance_clipped, t, y_t.shape)\n","        return posterior_mean, posterior_log_variance_clipped\n","\n","    def p_mean_variance(self, y_t, t, clip_denoised: bool, y_cond=None):\n","        # Predict mean and variance\n","        noise_level = get_index_from_list(self.gammas, t, x_shape=(1, 1)).to(y_t.device)\n","        y_0_hat = self.predict_start_from_noise(\n","            y_t, t=t, noise=self.denoise_fn(torch.cat([y_cond, y_t], dim=1), noise_level)\n","        )\n","        if clip_denoised:\n","            y_0_hat.clamp_(-1., 1.)\n","        model_mean, posterior_log_variance = self.q_posterior(y_0_hat=y_0_hat, y_t=y_t, t=t)\n","        return model_mean, posterior_log_variance\n","\n","    def q_sample(self, y_0, sample_gammas, noise=None):\n","        # Sample noisy data\n","        noise = noise if noise is not None else torch.randn_like(y_0)\n","        return (\n","            sample_gammas.sqrt() * y_0 +\n","            (1 - sample_gammas).sqrt() * noise\n","        )\n","\n","    def forward(self, y_0, y_cond=None, mask=None, noise=None):\n","        # Forward process\n","        b, *_ = y_0.shape\n","        t = torch.randint(1, self.num_timesteps, (b,), device=y_0.device).long()\n","        gamma_t1 = get_index_from_list(self.gammas, t - 1, x_shape=(1, 1))\n","        sqrt_gamma_t2 = get_index_from_list(self.gammas, t, x_shape=(1, 1))\n","        sample_gammas = (sqrt_gamma_t2 - gamma_t1) * torch.rand((b, 1), device=y_0.device) + gamma_t1\n","\n","        y_noisy = self.q_sample(y_0, sample_gammas.view(-1, 1, 1, 1), noise)\n","        if mask is not None:\n","            noise_hat = self.denoise_fn(torch.cat([y_cond, y_noisy * mask + (1. - mask) * y_0], dim=1), sample_gammas)\n","            loss = self.loss_fn(mask * noise, mask * noise_hat)\n","        else:\n","            noise_hat = self.denoise_fn(torch.cat([y_cond, y_noisy], dim=1), sample_gammas)\n","            loss = self.loss_fn(noise, noise_hat)\n","        return loss\n","\n","    @torch.no_grad()\n","    def p_sample(self, y_t, t, clip_denoised=True, y_cond=None):\n","        # Sample from posterior\n","        model_mean, model_log_variance = self.p_mean_variance(y_t=y_t, t=t, clip_denoised=clip_denoised, y_cond=y_cond)\n","        noise = torch.randn_like(y_t) if any(t > 0) else torch.zeros_like(y_t)\n","        return model_mean + noise * (0.5 * model_log_variance).exp()\n","\n","    @torch.no_grad()\n","    def restoration(self, y_cond, y_t=None, y_0=None, mask=None, sample_num=8):\n","        # Restore image from noisy input\n","        b, *_ = y_cond.shape\n","        sample_inter = self.num_timesteps // sample_num\n","        ret_arr = y_t\n","        y_t = y_t if y_t is not None else torch.randn_like(y_cond)\n","        for i in reversed(range(0, self.num_timesteps)):\n","            t = torch.full((b,), i, device=y_cond.device, dtype=torch.long)\n","            y_t = self.p_sample(y_t, t, y_cond=y_cond)\n","            if mask is not None:\n","                y_t = y_0 * (1. - mask) + mask * y_t\n","            if i % sample_inter == 0:\n","                ret_arr = torch.cat([ret_arr, y_t], dim=0)\n","        return y_t, ret_arr"],"metadata":{"id":"Mi34Bvp3zKWx","executionInfo":{"status":"aborted","timestamp":1743351971550,"user_tz":-420,"elapsed":340,"user":{"displayName":"Hanh Mai","userId":"06375785904431938117"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch.nn.functional as F\n","from torch import nn\n","\n","def mse_loss(output, target):\n","    \"\"\"\n","    Mean Squared Error Loss Function\n","    Args:\n","        output (torch.Tensor): Predicted values.\n","        target (torch.Tensor): Ground truth values.\n","    Returns:\n","        torch.Tensor: Computed MSE loss.\n","    \"\"\"\n","    return F.mse_loss(output, target)\n","\n","def mae(input, target):\n","    \"\"\"\n","    Mean Absolute Error Loss Function\n","    Args:\n","        input (torch.Tensor): Predicted values.\n","        target (torch.Tensor): Ground truth values.\n","    Returns:\n","        torch.Tensor: Computed MAE loss.\n","    \"\"\"\n","    with torch.no_grad():\n","        loss = nn.L1Loss()\n","        output = loss(input, target)\n","    return output"],"metadata":{"id":"ZuNP4uBrzKT3","executionInfo":{"status":"aborted","timestamp":1743351971576,"user_tz":-420,"elapsed":365,"user":{"displayName":"Hanh Mai","userId":"06375785904431938117"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import time\n","import torch\n","from tqdm import tqdm\n","\n","class Trainer:\n","    def __init__(self, model, optimizers, train_loader, val_loader, epochs, sample_num, device, save_model):\n","        self.model = model.to(device)\n","        self.optimizer = torch.optim.Adam(\n","            list(filter(lambda p: p.requires_grad, self.model.parameters())),\n","            **optimizers\n","        )\n","        self.model.set_loss(mse_loss)\n","        self.model.set_new_noise_schedule(device)\n","        self.sample_num = sample_num\n","        self.train_loader = train_loader\n","        self.val_loader = val_loader\n","        self.device = device\n","        self.epochs = epochs\n","        self.save_model = save_model + \"/best_model.pth\"\n","\n","    def train_step(self):\n","        \"\"\"\n","        Executes a single training step (epoch).\n","        \"\"\"\n","        losses = []\n","        for batch in tqdm(self.train_loader, desc=\"Training Batch\"):\n","            gt_image = batch['gt_image'].to(self.device)\n","            cond_image = batch['cond_image'].to(self.device)\n","            mask = batch['mask'].to(self.device)\n","\n","            self.optimizer.zero_grad()\n","            loss = self.model(gt_image, cond_image, mask=mask)\n","            loss.backward()\n","            losses.append(loss.item())\n","            self.optimizer.step()\n","\n","        return sum(losses) / len(losses)\n","\n","    def val_step(self):\n","        \"\"\"\n","        Executes a validation step to compute loss and MAE metrics.\n","        \"\"\"\n","        losses, metrics = [], []\n","        with torch.no_grad():\n","            for batch in tqdm(self.val_loader, desc=\"Validation Batch\"):\n","                gt_image = batch['gt_image'].to(self.device)\n","                cond_image = batch['cond_image'].to(self.device)\n","                mask = batch['mask'].to(self.device)\n","\n","                loss = self.model(gt_image, cond_image, mask=mask)\n","                output, _ = self.model.restoration(\n","                    cond_image, y_t=cond_image, y_0=gt_image, mask=mask, sample_num=self.sample_num\n","                )\n","                mae_score = mae(gt_image, output)\n","\n","                losses.append(loss.item())\n","                metrics.append(mae_score.item())\n","\n","        return sum(losses) / len(losses), sum(metrics) / len(metrics)\n","\n","    def train(self):\n","        \"\"\"\n","        Training loop across all epochs with validation and model checkpointing.\n","        \"\"\"\n","        best_mae = float('inf')\n","        for epoch in range(self.epochs):\n","            epoch_start_time = time.time()\n","\n","            train_loss = self.train_step()\n","            val_loss, val_mae = self.val_step()\n","\n","            if val_mae < best_mae:\n","                best_mae = val_mae\n","                torch.save(self.model.state_dict(), self.save_model)\n","\n","            print(\"-\" * 59)\n","            print(\n","                \"| End of epoch {:3d} | Time: {:5.2f}s | Train Loss: {:8.3f} | Valid Loss: {:8.3f} | Valid MAE: {:8.3f} |\".format(\n","                    epoch + 1, time.time() - epoch_start_time, train_loss, val_loss, val_mae\n","                )\n","            )\n","            print(\"-\" * 59)\n","\n","        self.model.load_state_dict(torch.load(self.save_model))\n","\n","# Define hyperparameters and configurations\n","epochs = 200\n","sample_num = 8\n","save_model = \"./save_model\"\n","optimizers = {\"lr\": 5e-5, \"weight_decay\": 0}\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","unet_config = {\n","    \"in_channel\": 6,\n","    \"out_channel\": 3,\n","    \"inner_channel\": 64,\n","    \"channel_mults\": [1, 2, 4, 8],\n","    \"attn_res\": [16],\n","    \"num_head_channels\": 32,\n","    \"res_blocks\": 2,\n","    \"dropout\": 0.2,\n","    \"image_size\": 256\n","}\n","\n","beta_schedule = {\n","    \"schedule\": \"linear\",\n","    \"n_timestep\": 20,\n","    \"linear_start\": 1e-4,\n","    \"linear_end\": 0.09\n","}\n","\n","inpainting_model = InpaintingGaussianDiffusion(unet_config, beta_schedule)\n","\n","# Initialize the trainer\n","trainer = Trainer(inpainting_model, optimizers, train_loader, val_loader, epochs, sample_num, device, save_model)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"Xay3HWQHzKQ-","executionInfo":{"status":"error","timestamp":1743351980003,"user_tz":-420,"elapsed":8425,"user":{"displayName":"Hanh Mai","userId":"06375785904431938117"}},"outputId":"8ea1aa01-3945-42d3-8580-105c6a0c6bc9"},"execution_count":2,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'InpaintingGaussianDiffusion' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-d18466b0176c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    110\u001b[0m }\n\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m \u001b[0minpainting_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInpaintingGaussianDiffusion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munet_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta_schedule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;31m# Initialize the trainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'InpaintingGaussianDiffusion' is not defined"]}]},{"cell_type":"code","source":["# Load the model and set the noise schedule\n","inpainting_model = InpaintingGaussianDiffusion(unet_config, beta_schedule)\n","inpainting_model.set_new_noise_schedule(device)\n","\n","# Load pre-trained weights\n","load_state = torch.load('./save_model/best_model_200.pth')\n","inpainting_model.load_state_dict(load_state, strict=True)\n","inpainting_model.eval().to(device)"],"metadata":{"id":"Z1htqbN_zKOM","executionInfo":{"status":"aborted","timestamp":1743351980005,"user_tz":-420,"elapsed":5,"user":{"displayName":"Hanh Mai","userId":"06375785904431938117"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Test image path\n","test_imgpath = img_paths[16]\n","\n","# Create test dataset\n","test_dataset = InpaintingDataset([test_imgpath], mask_mode='center')\n","\n","# Fetch a sample from the dataset\n","test_sample = next(iter(test_dataset))"],"metadata":{"id":"x6vcqxGpzKLa","executionInfo":{"status":"aborted","timestamp":1743351980008,"user_tz":-420,"elapsed":6,"user":{"displayName":"Hanh Mai","userId":"06375785904431938117"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def inference(model, test_sample):\n","    \"\"\"\n","    Perform inference using the trained model.\n","\n","    Args:\n","        model (nn.Module): Trained inpainting model.\n","        test_sample (dict): Test sample including condition image, mask, etc.\n","\n","    Returns:\n","        tuple: Output image and visuals (additional details, if applicable).\n","    \"\"\"\n","    with torch.no_grad():\n","        output, visuals = model.restoration(\n","            test_sample['cond_image'].unsqueeze(0).to(device),\n","            y_t=test_sample['cond_image'].unsqueeze(0).to(device),\n","            y_0=test_sample['cond_image'].unsqueeze(0).to(device),\n","            mask=test_sample['mask'].unsqueeze(0).to(device)\n","        )\n","    return output, visuals\n","\n","# Run inference\n","output, visuals = inference(inpainting_model, test_sample)"],"metadata":{"id":"ouMUkJe2zKIn","executionInfo":{"status":"aborted","timestamp":1743351980018,"user_tz":-420,"elapsed":7,"user":{"displayName":"Hanh Mai","userId":"06375785904431938117"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def show_tensor_image(image, show=True):\n","    \"\"\"\n","    Visualize a tensor image.\n","\n","    Args:\n","        image (torch.Tensor): Tensor image to be displayed.\n","        show (bool): Whether to display the image.\n","\n","    Returns:\n","        None: Displays the image.\n","    \"\"\"\n","    reverse_transforms = transforms.Compose([\n","        transforms.Lambda(lambda t: (t + 1) / 2),  # Unnormalize\n","        transforms.Lambda(lambda t: t.permute(1, 2, 0)),  # CHW to HWC\n","        transforms.Lambda(lambda t: t * 255.),  # Scale to [0, 255]\n","        transforms.Lambda(lambda t: t.numpy().astype(np.uint8)),  # Convert to numpy\n","        transforms.ToPILImage()  # Convert to PIL image\n","    ])\n","\n","    # Take the first image in the batch if batched\n","    if len(image.shape) == 4:\n","        image = image[0, :, :, :]\n","\n","    if show:\n","        plt.imshow(reverse_transforms(image))\n","        plt.axis('off')  # Remove axes for better visualization"],"metadata":{"id":"u3x33UHPzKFy","executionInfo":{"status":"aborted","timestamp":1743351980064,"user_tz":-420,"elapsed":44,"user":{"displayName":"Hanh Mai","userId":"06375785904431938117"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"yHfNvGD7zKC4","executionInfo":{"status":"aborted","timestamp":1743351980068,"user_tz":-420,"elapsed":8852,"user":{"displayName":"Hanh Mai","userId":"06375785904431938117"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"LTtvQBfXzKAF","executionInfo":{"status":"aborted","timestamp":1743351980070,"user_tz":-420,"elapsed":8853,"user":{"displayName":"Hanh Mai","userId":"06375785904431938117"}}},"execution_count":null,"outputs":[]}]}