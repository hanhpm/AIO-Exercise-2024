{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNcV7HKIYsb7W7EBU5FcIQH"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"m8WIc0K5EHzr"},"outputs":[],"source":["# Install required libraries\n","!pip install -q -U transformers datasets accelerate evaluate\n","\n","from datasets import load_dataset\n","\n","# Load the dataset\n","ds = load_dataset('thainq107/ntc-scv')"]},{"cell_type":"code","source":["# Import tokenizer\n","from transformers import AutoTokenizer\n","\n","# Define the model name\n","model_name = \"distilbert-base-uncased\"  # Change to \"bert-base-uncased\" if needed\n","\n","# Initialize tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(\n","    model_name,\n","    use_fast=True\n",")\n","\n","# Set max sequence length\n","max_seq_length = 100\n","max_seq_length = min(max_seq_length, tokenizer.model_max_length)\n","\n","# Preprocessing function\n","def preprocess_function(examples):\n","    result = tokenizer(\n","        examples[\"preprocessed_sentence\"],  # Column name in the dataset\n","        padding=\"max_length\",\n","        max_length=max_seq_length,\n","        truncation=True\n","    )\n","    result[\"label\"] = examples[\"label\"]\n","    return result\n","\n","# Apply the preprocessing pipeline\n","processed_dataset = ds.map(\n","    preprocess_function,\n","    batched=True,\n","    desc=\"Running tokenizer on dataset\",\n",")"],"metadata":{"id":"C9M5x6mQEh9n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import AutoConfig, AutoModelForSequenceClassification\n","\n","# Define the number of labels\n","num_labels = 2\n","\n","# Load model configuration\n","config = AutoConfig.from_pretrained(\n","    model_name,\n","    num_labels=num_labels,\n","    finetuning_task=\"text-classification\"\n",")\n","\n","# Load the model\n","model = AutoModelForSequenceClassification.from_pretrained(\n","    model_name,\n","    config=config\n",")"],"metadata":{"id":"RIg4DKy-Eh6y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import evaluate\n","\n","# Load accuracy metric\n","metric = evaluate.load(\"accuracy\")\n","\n","# Compute metrics function\n","def compute_metrics(eval_pred):\n","    predictions, labels = eval_pred\n","    predictions = np.argmax(predictions, axis=1)\n","    result = metric.compute(predictions=predictions, references=labels)\n","    return result"],"metadata":{"id":"2-xwkkyCEh3z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import TrainingArguments, Trainer\n","\n","# Define training arguments\n","training_args = TrainingArguments(\n","    output_dir=\"save_model\",               # Directory to save the model\n","    learning_rate=2e-5,                    # Learning rate\n","    per_device_train_batch_size=128,      # Batch size for training\n","    per_device_eval_batch_size=128,       # Batch size for evaluation\n","    num_train_epochs=10,                   # Number of training epochs\n","    eval_strategy=\"epoch\",                 # Evaluation strategy\n","    save_strategy=\"epoch\",                 # Save strategy\n","    load_best_model_at_end=True            # Load the best model at the end of training\n",")\n","\n","# Initialize the Trainer\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=processed_dataset[\"train\"],\n","    eval_dataset=processed_dataset[\"valid\"],\n","    compute_metrics=compute_metrics,\n","    tokenizer=tokenizer,\n",")\n","\n","# Train the model\n","trainer.train()"],"metadata":{"id":"JdahLcjTEh08"},"execution_count":null,"outputs":[]}]}