{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO2bVlHnh55UHckLoPyevvI"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"V8iiNWimEwjb"},"outputs":[],"source":["import torch\n","import torchvision.transforms as transforms\n","from torch.utils.data import DataLoader, random_split\n","import torch.optim as optim\n","from torchvision.datasets import ImageFolder\n","from torch import nn\n","import os\n","\n","# Download dataset (example code for Google Drive link)\n","!gdown 1vSevps_hV5zhVf6aWuN8X7dd-qSAIgcc\n","!unzip ./flower_photos.zip\n","\n","# Load dataset\n","data_path = \"./flower_photos\"\n","dataset = ImageFolder(root=data_path)\n","num_samples = len(dataset)\n","classes = dataset.classes\n","num_classes = len(dataset.classes)\n","\n","# Split dataset into train, validation, and test sets\n","TRAIN_RATIO, VALID_RATIO = 0.8, 0.1\n","n_train_examples = int(num_samples * TRAIN_RATIO)\n","n_valid_examples = int(num_samples * VALID_RATIO)\n","n_test_examples = num_samples - n_train_examples - n_valid_examples\n","\n","train_dataset, valid_dataset, test_dataset = random_split(\n","    dataset,\n","    [n_train_examples, n_valid_examples, n_test_examples]\n",")"]},{"cell_type":"code","source":["# Define image size\n","IMG_SIZE = 224\n","\n","# Data transformations for training and testing\n","train_transforms = transforms.Compose([\n","    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.RandomRotation(0.2),\n","    transforms.ToTensor(),\n","    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n","])\n","\n","test_transforms = transforms.Compose([\n","    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n","    transforms.ToTensor(),\n","    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n","])\n","\n","# Apply transformations to datasets\n","train_dataset.dataset.transform = train_transforms\n","valid_dataset.dataset.transform = test_transforms\n","test_dataset.dataset.transform = test_transforms"],"metadata":{"id":"ZFQBazaeF67N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define batch size\n","BATCH_SIZE = 512\n","\n","# Create DataLoaders\n","train_loader = DataLoader(\n","    train_dataset,\n","    shuffle=True,\n","    batch_size=BATCH_SIZE\n",")\n","\n","val_loader = DataLoader(\n","    valid_dataset,\n","    batch_size=BATCH_SIZE\n",")\n","\n","test_loader = DataLoader(\n","    test_dataset,\n","    batch_size=BATCH_SIZE\n",")"],"metadata":{"id":"c2Pz-yPpF85w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class TransformerEncoder(nn.Module):\n","    def __init__(self, embed_dim, num_heads, ff_dim, dropout=0.1):\n","        super().__init__()\n","        self.attn = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=num_heads, batch_first=True)\n","        self.ffn = nn.Sequential(\n","            nn.Linear(in_features=embed_dim, out_features=ff_dim, bias=True),\n","            nn.ReLU(),\n","            nn.Linear(in_features=ff_dim, out_features=embed_dim, bias=True)\n","        )\n","        self.layernorm_1 = nn.LayerNorm(normalized_shape=embed_dim, eps=1e-6)\n","        self.layernorm_2 = nn.LayerNorm(normalized_shape=embed_dim, eps=1e-6)\n","        self.dropout_1 = nn.Dropout(p=dropout)\n","        self.dropout_2 = nn.Dropout(p=dropout)\n","\n","    def forward(self, query, key, value):\n","        attn_output, _ = self.attn(query, key, value)\n","        attn_output = self.dropout_1(attn_output)\n","        out_1 = self.layernorm_1(query + attn_output)\n","        ffn_output = self.ffn(out_1)\n","        ffn_output = self.dropout_2(ffn_output)\n","        out_2 = self.layernorm_2(out_1 + ffn_output)\n","        return out_2\n","\n","class PatchPositionEmbedding(nn.Module):\n","    def __init__(self, image_size=224, embed_dim=512, patch_size=16, device='cpu'):\n","        super().__init__()\n","        self.conv1 = nn.Conv2d(in_channels=3, out_channels=embed_dim, kernel_size=patch_size, stride=patch_size, bias=False)\n","        scale = embed_dim ** -0.5\n","        self.positional_embedding = nn.Parameter(scale * torch.randn((image_size // patch_size) ** 2, embed_dim))\n","        self.device = device\n","\n","    def forward(self, x):\n","        x = self.conv1(x)  # shape = [*, width, grid, grid]\n","        x = x.reshape(x.shape[0], x.shape[1], -1)  # shape = [*, width, grid**2]\n","        x = x.permute(0, 2, 1)  # shape = [*, grid**2, width]\n","        x = x + self.positional_embedding.to(self.device)\n","        return x\n","\n","class VisionTransformerCls(nn.Module):\n","    def __init__(self, image_size, embed_dim, num_heads, ff_dim, dropout=0.1, device='cpu', num_classes=10, patch_size=16):\n","        super().__init__()\n","        self.embd_layer = PatchPositionEmbedding(image_size=image_size, embed_dim=embed_dim, patch_size=patch_size, device=device)\n","        self.transformer_layer = TransformerEncoder(embed_dim, num_heads, ff_dim, dropout)\n","        self.fc1 = nn.Linear(in_features=embed_dim, out_features=20)\n","        self.fc2 = nn.Linear(in_features=20, out_features=num_classes)\n","        self.dropout = nn.Dropout(p=dropout)\n","        self.relu = nn.ReLU()\n","\n","    def forward(self, x):\n","        output = self.embd_layer(x)\n","        output = self.transformer_layer(output, output, output)\n","        output = output[:, 0, :]\n","        output = self.dropout(output)\n","        output = self.fc1(output)\n","        output = self.dropout(output)\n","        output = self.fc2(output)\n","        return output"],"metadata":{"id":"nU23u1BzF826"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["image_size = 224\n","embed_dim = 512\n","num_heads = 4\n","ff_dim = 128\n","dropout = 0.1\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","model = VisionTransformerCls(\n","    image_size=image_size, embed_dim=embed_dim, num_heads=num_heads, ff_dim=ff_dim,\n","    dropout=dropout, num_classes=num_classes, device=device\n",")\n","model.to(device)\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.0005)\n","\n","num_epochs = 100\n","save_model = './vit_flowers'\n","os.makedirs(save_model, exist_ok=True)\n","model_name = 'vit_flowers'\n","\n","model, metrics = train(\n","    model, model_name, save_model, optimizer, criterion, train_loader, val_loader, num_epochs, device\n",")"],"metadata":{"id":"MpOcw5R9F8zz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import ViTForImageClassification\n","\n","id2label = {id: label for id, label in enumerate(classes)}\n","label2id = {label: id for id, label in id2label.items()}\n","\n","model = ViTForImageClassification.from_pretrained(\n","    'google/vit-base-patch16-224-in21k',\n","    num_labels=num_classes,\n","    id2label=id2label,\n","    label2id=label2id\n",")\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model.to(device)"],"metadata":{"id":"ZrNNAChDF8vm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import evaluate\n","import numpy as np\n","\n","metric = evaluate.load(\"accuracy\")\n","\n","def compute_metrics(eval_pred):\n","    predictions, labels = eval_pred\n","    predictions = np.argmax(predictions, axis=1)\n","    return metric.compute(predictions=predictions, references=labels)"],"metadata":{"id":"VMMAOpl0F8sK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import ViTImageProcessor, TrainingArguments, Trainer\n","\n","feature_extractor = ViTImageProcessor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n","\n","args = TrainingArguments(\n","    \"vit_flowers\",\n","    save_strategy=\"epoch\",\n","    evaluation_strategy=\"epoch\",\n","    learning_rate=2e-5,\n","    per_device_train_batch_size=32,\n","    per_device_eval_batch_size=32,\n","    num_train_epochs=10,\n","    weight_decay=0.01,\n","    load_best_model_at_end=True,\n","    metric_for_best_model=\"accuracy\",\n","    logging_dir='logs',\n","    remove_unused_columns=False,\n",")\n","\n","def collate_fn(examples):\n","    pixel_values = torch.stack([example[0] for example in examples])\n","    labels = torch.tensor([example[1] for example in examples])\n","    return {\"pixel_values\": pixel_values, \"labels\": labels}\n","\n","trainer = Trainer(\n","    model,\n","    args,\n","    train_dataset=train_dataset,\n","    eval_dataset=valid_dataset,\n","    data_collator=collate_fn,\n","    compute_metrics=compute_metrics,\n","    tokenizer=feature_extractor,\n",")"],"metadata":{"id":"G_r9F1zXGMMw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trainer.train()\n","outputs = trainer.predict(test_dataset)\n","print(outputs.metrics)"],"metadata":{"id":"dpE9AUytGMJw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"KNm-XN0bGMBJ"},"execution_count":null,"outputs":[]}]}